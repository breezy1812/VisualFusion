import os
import torch
import numpy as np
import random
from model_jit.SemLA import SemLA

# ============================================================================
# ğŸ”’ è¨­ç½®å®Œå…¨ç¢ºå®šæ€§ï¼ˆFP16 æ¨¡å¼ï¼‰
# ============================================================================
def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # cuDNN è¨­ç½®ï¼šç¢ºå®šæ€§æ¨¡å¼
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print("âœ… cuDNN deterministic mode enabled")
    
    # ç¦ç”¨ TF32ï¼ˆRTX 30 ç³»åˆ—çš„é—œéµè¨­ç½®ï¼‰
    if hasattr(torch.backends.cuda, 'matmul'):
        torch.backends.cuda.matmul.allow_tf32 = False
        print("âœ… CUDA matmul TF32 disabled")
    
    if hasattr(torch.backends.cudnn, 'allow_tf32'):
        torch.backends.cudnn.allow_tf32 = False
        print("âœ… cuDNN TF32 disabled")
    
    # è¨­ç½®ç’°å¢ƒè®Šé‡ï¼Œå¼·åˆ¶ç¦ç”¨ TF32
    os.environ['NVIDIA_TF32_OVERRIDE'] = '0'
    print("âœ… NVIDIA_TF32_OVERRIDE = 0")
    
    # ç¢ºå®šæ€§ç®—æ³•
    try:
        torch.use_deterministic_algorithms(True)
        print("âœ… Deterministic algorithms enabled")
    except Exception:
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
        print("âš ï¸  Fallback to CUBLAS_WORKSPACE_CONFIG")
    
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    print(f"âœ… Seeds set to {seed}, FP16 MODE ENABLED")
    print("   - TF32: DISABLED")
    print("   - FP16: ENFORCED")

# ============================================================================
# ä¸»è¦æµç¨‹ï¼šPyFP16 â†’ LibTorch FP16ï¼ˆç›´æ¥å°å‡º FP16ï¼‰
# ============================================================================
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # print("="*70)
    # print("ğŸ”¥ PyTorch to LibTorch FP16 Conversion Tool")
    # print("   æµç¨‹: PyTorch FP16 â†’ LibTorch FP16")
    # print("   â­ ç›´æ¥å°å‡º FP16 æ¨¡å‹ï¼Œç„¡ FP32 æ­¥é©Ÿ")
    # print("="*70)
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ï¼ˆFP16 æ¨¡å¼ï¼‰
    set_all_seeds(42)
    torch.set_grad_enabled(False)

    # ============================================================================
    # æ­¥é©Ÿ 1: è¼‰å…¥ PyTorch æ¨¡å‹ä¸¦è½‰æ›ç‚º FP16
    # ============================================================================
    # print("\nã€æ­¥é©Ÿ 1/2ã€‘è¼‰å…¥ PyTorch æ¨¡å‹ä¸¦è½‰æ›ç‚º FP16...")
    matcher = SemLA(device=device, fp=torch.float16)
    matcher.load_state_dict(torch.load("./reg.ckpt", map_location=device), strict=False)
    matcher.eval()
    matcher = matcher.to(device, dtype=torch.float16)
    
    # é©—è­‰æ‰€æœ‰åƒæ•¸éƒ½æ˜¯ FP16
    # print("ğŸ” é©—è­‰æ¨¡å‹åƒæ•¸é¡å‹...")
    # fp16_params = 0
    # fp32_params = 0
    # for name, param in matcher.named_parameters():
    #     if param.dtype == torch.float16:
    #         fp16_params += 1
    #     elif param.dtype == torch.float32:
    #         fp32_params += 1
    #         print(f"  âš ï¸  FP32 åƒæ•¸: {name}")
    # print(f"  âœ… FP16 åƒæ•¸: {fp16_params}")
    # if fp32_params > 0:
    #     print(f"  âš ï¸  FP32 åƒæ•¸: {fp32_params}ï¼ˆç•°å¸¸ï¼‰")

    # # é©—è­‰ BatchNorm å±¤
    # print("ğŸ” é©—è­‰ BatchNorm å±¤...")
    # bn_count = 0
    # for name, module in matcher.named_modules():
    #     if isinstance(module, torch.nn.BatchNorm2d):
    #         bn_count += 1
    #         module.eval()
    # print(f"âœ… æ‰¾åˆ° {bn_count} å€‹ BatchNorm2d å±¤ï¼Œå…¨éƒ¨å·²è¨­ç½®ç‚º eval æ¨¡å¼")

    # ============================================================================
    # æ­¥é©Ÿ 2: ä¿å­˜ LibTorch FP16 æ¨¡å‹
    # ============================================================================
    # print("\nã€æ­¥é©Ÿ 2/2ã€‘è½‰æ›ä¸¦ä¿å­˜ LibTorch FP16 æ¨¡å‹...")
    # set_all_seeds(42)
    
    # dummy forward åˆå§‹åŒ–ï¼ˆFP16ï¼‰
    dummy_input_rgb = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float16)
    dummy_input_ir  = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float16)
    with torch.no_grad():
        _ = matcher(dummy_input_rgb, dummy_input_ir)
    print("âœ… dummy forward å®Œæˆï¼Œæ¨¡å‹ buffer å·²åˆå§‹åŒ–ï¼ˆFP16ï¼‰")
    
    # è½‰æ›ç‚º TorchScript
    matcher_scripted_fp16 = torch.jit.script(matcher)
    fp16_output_path = "/circ330/forgithub/VisualFusion_libtorch/IR_Convert_v21_libtorch/model/SemLA_fp16.zip"
    torch.jit.save(matcher_scripted_fp16, fp16_output_path)
    print(f"âœ… LibTorch FP16 æ¨¡å‹å·²ä¿å­˜åˆ°: {fp16_output_path}")

    # ============================================================================
    # é©—è­‰æ¨¡å‹
    # ============================================================================
    # print("\nã€é©—è­‰ã€‘é©—è­‰ FP16 æ¨¡å‹æ¨è«–...")
    
    # # é‡æ–°è¼‰å…¥æ¨¡å‹ä»¥é©—è­‰
    # loaded_fp16_model = torch.jit.load(fp16_output_path, map_location=device)
    # loaded_fp16_model.eval()
    
    # set_all_seeds(42)
    # test_input_rgb = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float16)
    # test_input_ir  = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float16)

    # with torch.no_grad():
    #     # FP16 æ¨¡å‹æ¨è«–
    #     fp16_out = loaded_fp16_model(test_input_rgb, test_input_ir)
        
    #     # é©—è­‰è¼¸å‡º
    #     print("\nğŸ“Š FP16 æ¨¡å‹è¼¸å‡ºé©—è­‰:")
    #     for i, output in enumerate(fp16_out):
    #         print(f"output[{i}]: shape={output.shape}, dtype={output.dtype}")
    #     print("âœ… FP16 æ¨¡å‹æ¨è«–æˆåŠŸ")

    # print("\n" + "="*70)
    # print("âœ… è½‰æ›å®Œæˆï¼")
    # print(f"  - FP16 æ¨¡å‹: {fp16_output_path}")
    # print("")
    # print("â­ FP16 æ¨¡å‹ä½¿ç”¨æ–¹å¼ï¼š")
    # print("  1. åœ¨ C++ LibTorch ä¸­ç›´æ¥è¼‰å…¥æ­¤ FP16 æ¨¡å‹")
    # print("  2. è¼¸å…¥è³‡æ–™è½‰ç‚º FP16ï¼šinput.to(torch::kHalf)")
    # print("  3. ç„¡éœ€é¡å¤–è½‰æ›ï¼Œç›´æ¥æ¨è«–")
    # print("")
    # print("ğŸ“ C++ ç¯„ä¾‹ä»£ç¢¼ï¼š")
    # print("  torch::jit::script::Module module = torch::jit::load(\"SemLA_fp16.zip\");")
    # print("  module.to(torch::kCUDA);")
    # print("  // æ¨¡å‹å·²æ˜¯ FP16ï¼Œç„¡éœ€é¡å¤–è½‰æ›")
    # print("")
    # print("  auto input_rgb_fp16 = input_rgb.to(torch::kHalf);")
    # print("  auto input_ir_fp16 = input_ir.to(torch::kHalf);")
    # print("  auto outputs = module.forward({input_rgb_fp16, input_ir_fp16});")
    # print("="*70)

# ============================================================================
if __name__ == "__main__":
    main()
