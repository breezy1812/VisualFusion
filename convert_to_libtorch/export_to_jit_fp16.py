import os
import torch
import numpy as np
import random
from model_jit.SemLA import SemLA

# ============================================================================
# ğŸ”’ è¨­ç½®å®Œå…¨ç¢ºå®šæ€§ï¼ˆFP32 æ¨¡å¼ï¼‰
# ============================================================================
def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # cuDNN è¨­ç½®ï¼šç¢ºå®šæ€§æ¨¡å¼
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print("âœ… cuDNN deterministic mode enabled")
    
    # ç¦ç”¨ TF32ï¼ˆRTX 30 ç³»åˆ—çš„é—œéµè¨­ç½®ï¼‰
    if hasattr(torch.backends.cuda, 'matmul'):
        torch.backends.cuda.matmul.allow_tf32 = False
        print("âœ… CUDA matmul TF32 disabled")
    
    if hasattr(torch.backends.cudnn, 'allow_tf32'):
        torch.backends.cudnn.allow_tf32 = False
        print("âœ… cuDNN TF32 disabled")
    
    # è¨­ç½®ç’°å¢ƒè®Šé‡ï¼Œå¼·åˆ¶ç¦ç”¨ TF32
    os.environ['NVIDIA_TF32_OVERRIDE'] = '0'
    print("âœ… NVIDIA_TF32_OVERRIDE = 0")
    
    # ç¢ºå®šæ€§ç®—æ³•
    try:
        torch.use_deterministic_algorithms(True)
        print("âœ… Deterministic algorithms enabled")
    except Exception:
        os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
        print("âš ï¸  Fallback to CUBLAS_WORKSPACE_CONFIG")
    
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    print(f"âœ… Seeds set to {seed}, FP32 MODE ENABLED")
    print("   - TF32: DISABLED")
    print("   - FP32: ENFORCED")

# ============================================================================
# ä¸»è¦æµç¨‹ï¼šPyFP32 â†’ LibTorch FP32ï¼ˆæ¥­ç•Œæ¨è–¦æ–¹æ¡ˆï¼‰
# FP16 è½‰æ›å°‡åœ¨ C++ LibTorch ç«¯å‹•æ…‹åŸ·è¡Œ
# ============================================================================
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("="*70)
    print("ğŸ”¥ PyTorch to LibTorch FP32 Conversion Tool")
    print("   æµç¨‹: PyTorch FP32 â†’ LibTorch FP32")
    print("   â­ FP16 è½‰æ›å°‡åœ¨ C++ ç«¯åŸ·è¡Œ (æ¥­ç•Œæ¨è–¦æ–¹æ¡ˆ)")
    print("="*70)
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ï¼ˆFP32 æ¨¡å¼ï¼‰
    set_all_seeds(42)
    torch.set_grad_enabled(False)

    # ============================================================================
    # æ­¥é©Ÿ 1: è¼‰å…¥ PyTorch FP32 æ¨¡å‹
    # ============================================================================
    print("\nã€æ­¥é©Ÿ 1/2ã€‘è¼‰å…¥ PyTorch FP32 æ¨¡å‹...")
    matcher = SemLA(device=device, fp=torch.float32)
    matcher.load_state_dict(torch.load("./reg.ckpt", map_location=device), strict=False)
    matcher.eval()
    matcher = matcher.to(device, dtype=torch.float32)
    
    # é©—è­‰æ‰€æœ‰åƒæ•¸éƒ½æ˜¯ FP32
    print("ğŸ” é©—è­‰æ¨¡å‹åƒæ•¸é¡å‹...")
    fp32_params = 0
    fp16_params = 0
    for name, param in matcher.named_parameters():
        if param.dtype == torch.float32:
            fp32_params += 1
        elif param.dtype == torch.float16:
            fp16_params += 1
            print(f"  âš ï¸  FP16 åƒæ•¸: {name}")
    print(f"  âœ… FP32 åƒæ•¸: {fp32_params}")
    if fp16_params > 0:
        print(f"  âš ï¸  FP16 åƒæ•¸: {fp16_params}ï¼ˆå°‡ä¿æŒç‚º FP32ï¼‰")

    # é©—è­‰ BatchNorm å±¤
    print("ğŸ” é©—è­‰ BatchNorm å±¤...")
    bn_count = 0
    for name, module in matcher.named_modules():
        if isinstance(module, torch.nn.BatchNorm2d):
            bn_count += 1
            module.eval()
    print(f"âœ… æ‰¾åˆ° {bn_count} å€‹ BatchNorm2d å±¤ï¼Œå…¨éƒ¨å·²è¨­ç½®ç‚º eval æ¨¡å¼")

    # ============================================================================
    # æ­¥é©Ÿ 2: ä¿å­˜ LibTorch FP32 æ¨¡å‹ï¼ˆåƒ…æ­¤è€Œå·²ï¼‰
    # ============================================================================
    print("\nã€æ­¥é©Ÿ 2/2ã€‘è½‰æ›ä¸¦ä¿å­˜ LibTorch FP32 æ¨¡å‹...")
    set_all_seeds(42)
    
    # dummy forward åˆå§‹åŒ–
    dummy_input_rgb = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float32)
    dummy_input_ir  = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float32)
    with torch.no_grad():
        _ = matcher(dummy_input_rgb, dummy_input_ir)
    print("âœ… dummy forward å®Œæˆï¼Œæ¨¡å‹ buffer å·²åˆå§‹åŒ–")
    
    # è½‰æ›ç‚º TorchScript
    matcher_scripted_fp32 = torch.jit.script(matcher)
    fp32_output_path = "/circ330/forgithub/VisualFusion_libtorch/IR_Convert_v21_libtorch/model/SemLA_fp32.zip"
    torch.jit.save(matcher_scripted_fp32, fp32_output_path)
    print(f"âœ… LibTorch FP32 æ¨¡å‹å·²ä¿å­˜åˆ°: {fp32_output_path}")

    # ============================================================================
    # é©—è­‰æ¨¡å‹
    # ============================================================================
    print("\nã€é©—è­‰ã€‘é©—è­‰ FP32 æ¨¡å‹æ¨è«–...")
    
    # é‡æ–°è¼‰å…¥æ¨¡å‹ä»¥é©—è­‰
    loaded_fp32_model = torch.jit.load(fp32_output_path, map_location=device)
    loaded_fp32_model.eval()
    
    set_all_seeds(42)
    test_input_rgb = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float32)
    test_input_ir  = torch.randn(1, 1, 240, 320, device=device, dtype=torch.float32)

    with torch.no_grad():
        # FP32 æ¨¡å‹æ¨è«–
        fp32_out = loaded_fp32_model(test_input_rgb, test_input_ir)
        
        # é©—è­‰è¼¸å‡º
        print("\nğŸ“Š FP32 æ¨¡å‹è¼¸å‡ºé©—è­‰:")
        for i, output in enumerate(fp32_out):
            print(f"output[{i}]: shape={output.shape}, dtype={output.dtype}")
        print("âœ… FP32 æ¨¡å‹æ¨è«–æˆåŠŸ")

    print("\n" + "="*70)
    print("âœ… è½‰æ›å®Œæˆï¼")
    print(f"  - FP32 æ¨¡å‹: {fp32_output_path}")
    print("")
    print("â­ æ¥­ç•Œæ¨è–¦çš„ FP16 ä½¿ç”¨æ–¹å¼ï¼š")
    print("  1. åœ¨ C++ LibTorch ä¸­è¼‰å…¥æ­¤ FP32 æ¨¡å‹")
    print("  2. ä½¿ç”¨ module.to(torch::kHalf) å‹•æ…‹è½‰æ›ç‚º FP16")
    print("  3. è¼¸å…¥è³‡æ–™ä¹Ÿè½‰ç‚º FP16ï¼šinput.to(torch::kHalf)")
    print("")
    print("ğŸ“ C++ ç¯„ä¾‹ä»£ç¢¼ï¼š")
    print("  torch::jit::script::Module module = torch::jit::load(\"SemLA_jit_cuda_fp32.zip\");")
    print("  module.to(torch::kCUDA);")
    print("  module.to(torch::kHalf);  // å‹•æ…‹è½‰ FP16")
    print("")
    print("  auto input_rgb_fp16 = input_rgb.to(torch::kHalf);")
    print("  auto input_ir_fp16 = input_ir.to(torch::kHalf);")
    print("  auto outputs = module.forward({input_rgb_fp16, input_ir_fp16});")
    print("="*70)

# ============================================================================
if __name__ == "__main__":
    main()
