#!/usr/bin/env python3
"""
PyTorch to TensorRT FP32 Conversion Script
å¾ PyTorch æ¨¡å‹è½‰æ›ç‚º TensorRT FP32 å¼•æ“ï¼ˆä½¿ç”¨ trtexecï¼Œæ›´ç©©å®šï¼‰

åŸºæ–¼: build_trt_engine_fp32.sh
ä½¿ç”¨ trtexec å‘½ä»¤åˆ—å·¥å…·ï¼ˆæ¯” Python API æ›´ç©©å®šï¼‰

Usage:
    python export_onnx2tensorRT.py
"""

import os
import sys
import subprocess
import argparse

def main():
    """å›ºå®šè¼¸å‡º FP32 TensorRT æ¨¡å‹ï¼ˆä½¿ç”¨ trtexecï¼‰"""
    parser = argparse.ArgumentParser(description='Convert PyTorch model to TensorRT FP32 engine using trtexec')
    parser.add_argument('--model', type=str,
                       default='./reg.ckpt',
                       help='Path to PyTorch model checkpoint (default: ./reg.ckpt)')
    parser.add_argument('--onnx-output', type=str, 
                       default='../tensorRT_nx/model/NX/Nxfp32.onnx',
                       help='Path to temporary ONNX file')
    parser.add_argument('--trt-output', type=str, 
                       default='../tensorRT_nx/model/NX/trt_Nx_fp32.engine',
                       help='Path to output TensorRT engine')
    parser.add_argument('--opset', type=int, default=12,
                       help='ONNX opset version (default: 12)')
    parser.add_argument('--trtexec-path', type=str,
                       default='/usr/src/tensorrt/bin/trtexec',
                       help='Path to trtexec binary')

    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ¯ PyTorch to TensorRT FP32 Conversion Tool (using trtexec)")
    print("=" * 70)
    print("ğŸ“‹ Configuration:")
    print(f"  ğŸ§  PyTorch model: {args.model}")
    print(f"  ğŸ“„ ONNX output: {args.onnx_output}")
    print(f"  ğŸ’¾ TRT engine: {args.trt_output}")
    print(f"  ğŸš€ Precision: FP32 (TF32 disabled)")
    print(f"  ğŸ“¦ ONNX OpSet: {args.opset}")
    print(f"  ğŸ”§ trtexec: {args.trtexec_path}")
    print("=" * 70)
    
    # æ­¥é©Ÿ 1: PyTorch â†’ ONNX
    print("\n" + "=" * 70)
    print("æ­¥é©Ÿ 1/2: è½‰æ› PyTorch â†’ ONNX...")
    print("=" * 70)
    
    if not convert_pytorch_to_onnx(args.model, args.onnx_output, args.opset):
        print("\nâŒ PyTorch â†’ ONNX è½‰æ›å¤±æ•—")
        return 1
    
    # æ­¥é©Ÿ 2: ONNX â†’ TensorRT (ä½¿ç”¨ trtexec)
    print("\n" + "=" * 70)
    print("æ­¥é©Ÿ 2/2: è½‰æ› ONNX â†’ TensorRT (ä½¿ç”¨ trtexec)...")
    print("=" * 70)
    
    if not convert_onnx_to_trt(args.onnx_output, args.trt_output, args.trtexec_path):
        print("\nâŒ ONNX â†’ TensorRT è½‰æ›å¤±æ•—")
        return 1
    
    print("\n" + "=" * 70)
    print("âœ… è½‰æ›å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ“Œ TensorRT FP32 engine: {args.trt_output}")
    print("ğŸ”§ Update your configuration files to use this new engine.")
    print("=" * 70)
    
    return 0

def convert_pytorch_to_onnx(model_path, onnx_path, opset_version):
    """æ­¥é©Ÿ 1: ä½¿ç”¨ Python å°‡ PyTorch è½‰æ›ç‚º ONNX"""
    
    python_script = f'''
import os
os.environ["NVIDIA_TF32_OVERRIDE"] = "0"

import torch
import onnx
import onnxsim

# è¨­å®šç¢ºå®šæ€§
torch.manual_seed(42)
torch.use_deterministic_algorithms(True, warn_only=True)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

if hasattr(torch.backends.cuda, 'matmul'):
    torch.backends.cuda.matmul.allow_tf32 = False
if hasattr(torch.backends.cudnn, 'allow_tf32'):
    torch.backends.cudnn.allow_tf32 = False

print("âœ… TF32 å·²ç¦ç”¨ (NVIDIA_TF32_OVERRIDE=0)")

# è¼‰å…¥æ¨¡å‹
from model_jit.SemLA import SemLA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
fpMode = torch.float32

print(f"ä½¿ç”¨è¨­å‚™: {{device}}")
print("æ­£åœ¨è¼‰å…¥æ¨¡å‹ (FP32)...")

matcher = SemLA(device=device, fp=fpMode)
matcher.load_state_dict(torch.load("{model_path}", map_location=device), strict=False)
matcher = matcher.eval().to(device, dtype=fpMode)
torch.set_grad_enabled(False)

print("âœ… æ¨¡å‹å·²è¼‰å…¥ (FP32)")

# å»ºç«‹è¼¸å…¥
width = 320
height = 240
torch_input_1 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)
torch_input_2 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)

# è½‰æ›ç‚º ONNX
onnx_path = "{onnx_path}"
print(f"è½‰æ›ç‚º ONNX (OpSet {opset_version})...")

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
os.makedirs(os.path.dirname(onnx_path), exist_ok=True)

torch.onnx.export(
    matcher,
    (torch_input_1, torch_input_2),
    onnx_path,
    verbose=False,
    opset_version={opset_version},
    input_names=["vi_img", "ir_img"],
    output_names=["mkpt0", "mkpt1"],
    do_constant_folding=True,
    dynamic_axes=None,
)
print(f"âœ… ONNX è½‰æ›å®Œæˆ")

# ç°¡åŒ– ONNX
print("ç°¡åŒ– ONNX æ¨¡å‹...")
model_onnx = onnx.load(onnx_path)
model_simp, check = onnxsim.simplify(model_onnx)
if not check:
    print("âš ï¸  ONNX ç°¡åŒ–å¤±æ•—ï¼Œä½¿ç”¨æœªç°¡åŒ–ç‰ˆæœ¬")
    model_simp = model_onnx

onnx.save(model_simp, onnx_path)
print(f"âœ… ONNX å·²å„²å­˜: {{onnx_path}}")

# é©—è­‰ ONNX
try:
    onnx.checker.check_model(model_simp)
    print("âœ… ONNX é©—è­‰é€šé")
except Exception as e:
    print(f"âš ï¸  ONNX é©—è­‰è­¦å‘Š: {{e}}")

# åˆ—å‡ºæ‰€æœ‰é‹ç®—ç¬¦
ops = set()
for node in model_simp.graph.node:
    ops.add(node.op_type)
print(f"\\nğŸ“‹ ONNX é‹ç®—ç¬¦ ({{len(ops)}} ç¨®):")
for op in sorted(ops):
    print(f"  - {{op}}")
'''
    
    try:
        # åŸ·è¡Œ Python è…³æœ¬
        result = subprocess.run(
            ['python3', '-c', python_script],
            capture_output=False,
            text=True,
            check=True
        )
        
        # æª¢æŸ¥ ONNX æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(onnx_path):
            print(f"âŒ ONNX æª”æ¡ˆä¸å­˜åœ¨: {onnx_path}")
            return False
            
        print(f"âœ… ONNX æª”æ¡ˆå·²å»ºç«‹: {onnx_path}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ PyTorch â†’ ONNX è½‰æ›å¤±æ•—")
        print(f"   éŒ¯èª¤: {e}")
        return False
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def convert_onnx_to_trt(onnx_path, trt_path, trtexec_path):
    """æ­¥é©Ÿ 2: ä½¿ç”¨ trtexec å°‡ ONNX è½‰æ›ç‚º TensorRT"""
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸
    env = os.environ.copy()
    env["NVIDIA_TF32_OVERRIDE"] = "0"
    env["LD_LIBRARY_PATH"] = "/circ330/TensorRT-8.4.3.1/lib:" + env.get("LD_LIBRARY_PATH", "")
    
    print(f"âœ… ç’°å¢ƒè®Šæ•¸: NVIDIA_TF32_OVERRIDE=0")
    print(f"âœ… LD_LIBRARY_PATH: {env['LD_LIBRARY_PATH']}")
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(os.path.dirname(trt_path), exist_ok=True)
    
    print("\nğŸ”¨ ä½¿ç”¨ trtexec å»ºç«‹ TensorRT engine...")
    print(f"   - è¼¸å…¥: {onnx_path}")
    print(f"   - è¼¸å‡º: {trt_path}")
    print(f"   - ç²¾åº¦: FP32 (ç¦ç”¨ TF32)")
    print("")
    
    # æ§‹å»º trtexec å‘½ä»¤
    cmd = [
        trtexec_path,
        f"--onnx={onnx_path}",
        f"--saveEngine={trt_path}",
        "--noTF32",
        "--verbose",
        "--dumpLayerInfo"
    ]
    
    log_file = "../trt_conversion_fp32.log"
    
    try:
        # åŸ·è¡Œ trtexecï¼ŒåŒæ™‚è¼¸å‡ºåˆ° console å’Œ log æª”æ¡ˆ
        with open(log_file, 'w') as f:
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                env=env,
                bufsize=1
            )
            
            # å³æ™‚è¼¸å‡ºä¸¦å¯«å…¥ log
            for line in process.stdout:
                print(line, end='')
                f.write(line)
            
            process.wait()
            
            if process.returncode != 0:
                print(f"\nâŒ trtexec è¿”å›éŒ¯èª¤ç¢¼: {process.returncode}")
                return False
        
        # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
        if os.path.exists(trt_path):
            file_size = os.path.getsize(trt_path) / (1024 * 1024)
            print(f"\nâœ… TensorRT Engine å·²å»ºç«‹")
            print(f"ğŸ“ æª”æ¡ˆ: {trt_path}")
            print(f"ğŸ“ å¤§å°: {file_size:.2f} MB")
            print(f"ğŸ“ å®Œæ•´æ—¥èªŒ: {log_file}")
            return True
        else:
            print(f"\nâŒ TensorRT Engine æª”æ¡ˆä¸å­˜åœ¨: {trt_path}")
            print(f"ğŸ“ è«‹æª¢æŸ¥æ—¥èªŒ: {log_file}")
            return False
            
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ° trtexec: {trtexec_path}")
        print("   è«‹ç¢ºèª TensorRT è·¯å¾‘æ­£ç¢º")
        return False
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

if __name__ == "__main__":
    sys.exit(main())
