import torch
import os
import onnx
from onnxconverter_common import float16

from model_jit.SemLA import SemLA

print("=== SemLA ONNX FP16 è½‰æ›è…³æœ¬ ===")

# ä½¿ç”¨CUDAä¾†ç²å¾—æœ€ä½³æ€§èƒ½
device = torch.device("cuda")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

# å…ˆä»¥ FP32 è¼‰å…¥æ¨¡å‹
fpMode = torch.float32
print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
matcher = SemLA(device=device, fp=fpMode)
matcher.load_state_dict(torch.load(f"./reg.ckpt", map_location=device), strict=False)
matcher = matcher.eval().to(device, dtype=fpMode)

# ä½¿ç”¨èˆ‡é…ç½®æ–‡ä»¶ç›¸ç¬¦çš„å°ºå¯¸
width = 320
height = 240

print(f"å»ºç«‹è¼¸å…¥å¼µé‡ï¼Œå°ºå¯¸: {height}x{width}")
torch_input_1 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)
torch_input_2 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
output_dir = "../Onnx/model/onnxModel"
os.makedirs(output_dir, exist_ok=True)

# å…ˆå°å‡ºFP32 ONNXæ¨¡å‹
fp32_output_path = f"{output_dir}/SemLA_onnx_{width}x{height}_fp32_temp.onnx"
fp16_output_path = f"{output_dir}/zETOfp16op12_fp16_{device}.onnx"

print(f"æ­¥é©Ÿ1: è½‰æ›ç‚ºFP32 ONNXæ¨¡å‹...")
print(f"è‡¨æ™‚è·¯å¾‘: {fp32_output_path}")

try:
    torch.onnx.export(
        matcher,
        (torch_input_1, torch_input_2),
        fp32_output_path,
        verbose=False,
        opset_version=12,  # ä½¿ç”¨è¼ƒæ–°ç‰ˆæœ¬æ”¯æ´æ›´å¤šæ“ä½œ
        input_names=["vi_img", "ir_img"],
        output_names=["mkpt0", "mkpt1", "leng1", "leng2"],
        do_constant_folding=True,
        # ç§»é™¤dynamic_axesï¼Œä½¿ç”¨å›ºå®šå°ºå¯¸
    )
    print("âœ… FP32 ONNXæ¨¡å‹è½‰æ›å®Œæˆ")
    
    # é©—è­‰FP32æ¨¡å‹
    onnx_model = onnx.load(fp32_output_path)
    onnx.checker.check_model(onnx_model)
    print("âœ… FP32 ONNXæ¨¡å‹é©—è­‰é€šé")
    
except Exception as e:
    print(f"âŒ FP32 ONNXè½‰æ›å¤±æ•—: {e}")
    exit(1)

print(f"æ­¥é©Ÿ2: è½‰æ›ç‚ºFP16 ONNXæ¨¡å‹...")
print(f"æœ€çµ‚è·¯å¾‘: {fp16_output_path}")

try:
    # è¼‰å…¥FP32æ¨¡å‹ä¸¦è½‰æ›ç‚ºFP16
    fp32_model = onnx.load(fp32_output_path)
    
    # è½‰æ›ç‚ºFP16ï¼Œä¿æŒè¼¸å…¥ç‚ºFP32
    fp16_model = float16.convert_float_to_float16(
        fp32_model, 
        keep_io_types=True  # ä¿æŒè¼¸å…¥è¼¸å‡ºç‚ºFP32ä»¥æé«˜å…¼å®¹æ€§
    )
    
    # å„²å­˜FP16æ¨¡å‹
    onnx.save(fp16_model, fp16_output_path)
    print("âœ… FP16 ONNXæ¨¡å‹è½‰æ›å®Œæˆ")
    
    # é©—è­‰FP16æ¨¡å‹
    onnx.checker.check_model(fp16_model)
    print("âœ… FP16 ONNXæ¨¡å‹é©—è­‰é€šé")
    
    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    os.remove(fp32_output_path)
    print("ğŸ§¹ æ¸…ç†è‡¨æ™‚æª”æ¡ˆå®Œæˆ")
    
except Exception as e:
    print(f"âŒ FP16è½‰æ›å¤±æ•—: {e}")
    print("å¯èƒ½éœ€è¦å®‰è£: pip install onnxconverter-common")
    exit(1)

# æª¢æŸ¥æª”æ¡ˆå¤§å°æ¯”è¼ƒ
if os.path.exists(fp16_output_path):
    file_size = os.path.getsize(fp16_output_path) / (1024*1024)  # MB
    print(f"ğŸ“Š FP16æ¨¡å‹å¤§å°: {file_size:.2f} MB")

print("ğŸ¯ FP16 ONNXæ¨¡å‹è½‰æ›å®Œæˆï¼")
print(f"æ¨¡å‹å·²å„²å­˜åˆ°: {fp16_output_path}")
print("ğŸ¯ å»ºè­°æ›´æ–°config.jsonä¸­çš„model_pathç‚º:")
print(f'    "{fp16_output_path}"')

# æä¾›æ¸¬è©¦å»ºè­°
print("\nğŸ’¡ æ¸¬è©¦å»ºè­°:")
print("1. æ›´æ–°config.jsonä½¿ç”¨æ–°çš„FP16æ¨¡å‹è·¯å¾‘")
print("2. ç¢ºä¿æ¨è«–ç’°å¢ƒæ”¯æ´FP16æ“ä½œ")
print("3. GPUç’°å¢ƒä¸‹FP16å¯èƒ½æä¾›æ›´å¥½çš„æ€§èƒ½")
print("4. æ¯”è¼ƒFP32èˆ‡FP16çš„æ¨è«–ç²¾åº¦å·®ç•°")