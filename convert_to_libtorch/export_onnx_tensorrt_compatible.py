#!/usr/bin/env python3
"""
æ”¹é€²ç‰ˆ ONNX å°Žå‡ºè…³æœ¬ - é¿å… TensorRT INT64 è­¦å‘Š
TODO:ä¸‹é€±
ç›´æŽ¥è½‰æ›æˆonnx int32ç‰ˆæœ¬ï¼Œä¹‹å¾Œæ‰åŽ»ç”¨export_onnx2tensorRT.pyè½‰æˆTensorRT
"""

import torch
import os
import onnx
from model_jit.SemLA import SemLA

print("=== SemLA ONNX å°Žå‡º (TensorRT ç›¸å®¹ç‰ˆæœ¬) ===")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

# è¼‰å…¥æ¨¡åž‹
fpMode = torch.float32
matcher = SemLA(device=device, fp=fpMode)
matcher.load_state_dict(torch.load("./reg.ckpt", map_location=device), strict=False)
matcher = matcher.eval().to(device, dtype=fpMode)

width = 320
height = 240

torch_input_1 = torch.randn(1, 1, height, width).to(device)
torch_input_2 = torch.randn(1, 1, height, width).to(device)

output_dir = "../Onnx/model/onnxModel"
os.makedirs(output_dir, exist_ok=True)

output_path = f"{output_dir}/SemLA_onnx_{width}x{height}_tensorrt_int32.onnx"

print(f"å°Žå‡º TensorRT ç›¸å®¹çš„ ONNX æ¨¡åž‹...")
print(f"è¼¸å‡ºè·¯å¾‘: {output_path}")

# å°Žå‡º ONNX æ¨¡åž‹ï¼ŒæŒ‡å®šè¼¸å‡ºé¡žåž‹é¿å… INT64 å•é¡Œ
torch.onnx.export(
    matcher,
    (torch_input_1, torch_input_2),
    output_path,
    verbose=False,
    opset_version=12,  # ä½¿ç”¨è¼ƒç©©å®šçš„ç‰ˆæœ¬
    input_names=["vi_img", "ir_img"],
    output_names=["mkpt0", "mkpt1", "leng1", "leng2"],
    do_constant_folding=True,
    # ç¢ºä¿è¼¸å‡ºé¡žåž‹å…¼å®¹æ€§
    operator_export_type=torch.onnx.OperatorExportTypes.ONNX,
)

print("âœ… ONNX æ¨¡åž‹å°Žå‡ºå®Œæˆ")

# å¾Œè™•ç†ï¼šæª¢æŸ¥ä¸¦ä¿®æ­£å¯èƒ½çš„ INT64 å•é¡Œ
try:
    import onnx
    from onnx import helper, TensorProto
    
    print("ðŸ”§ æª¢æŸ¥ä¸¦ä¿®æ­£ ONNX æ¨¡åž‹ä¸­çš„è³‡æ–™é¡žåž‹...")
    
    model = onnx.load(output_path)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ INT64 è¼¸å‡ºï¼Œå¦‚æœ‰å‰‡è½‰æ›ç‚º INT32
    modified = False
    for output in model.graph.output:
        if output.type.tensor_type.elem_type == TensorProto.INT64:
            print(f"  ä¿®æ­£è¼¸å‡º {output.name}: INT64 -> INT32")
            output.type.tensor_type.elem_type = TensorProto.INT32
            modified = True
    
    # æª¢æŸ¥ç¯€é»žä¸­çš„å±¬æ€§
    for node in model.graph.node:
        for attr in node.attribute:
            if attr.type == onnx.AttributeProto.INTS:
                # æª¢æŸ¥æ•´æ•¸åˆ—è¡¨æ˜¯å¦è¶…å‡º INT32 ç¯„åœ
                int_vals = list(attr.ints)
                if any(val > 2147483647 or val < -2147483648 for val in int_vals):
                    print(f"  è­¦å‘Šï¼šç¯€é»ž {node.name} åŒ…å«è¶…å‡º INT32 ç¯„åœçš„å€¼")
    
    if modified:
        # å„²å­˜ä¿®æ­£å¾Œçš„æ¨¡åž‹
        onnx.save(model, output_path)
        print("âœ… ONNX æ¨¡åž‹è³‡æ–™é¡žåž‹ä¿®æ­£å®Œæˆ")
    
    # é©—è­‰æ¨¡åž‹
    onnx.checker.check_model(model)
    print("âœ… ONNX æ¨¡åž‹é©—è­‰é€šéŽ")
    
except Exception as e:
    print(f"âš ï¸  æ¨¡åž‹å¾Œè™•ç†è­¦å‘Š: {e}")

# æª¢æŸ¥æª”æ¡ˆå¤§å°
file_size = os.path.getsize(output_path) / (1024*1024)
print(f"ðŸ“Š æ¨¡åž‹å¤§å°: {file_size:.2f} MB")
print(f"ðŸŽ¯ TensorRT ç›¸å®¹çš„ ONNX æ¨¡åž‹å·²å„²å­˜åˆ°: {output_path}")

print("\nðŸ’¡ ä½¿ç”¨å»ºè­°:")
print("1. æ­¤ç‰ˆæœ¬æ‡‰è©²èƒ½æ¸›å°‘ TensorRT è½‰æ›æ™‚çš„ INT64 è­¦å‘Š")
print("2. å¯ä»¥ç”¨æ­¤æ¨¡åž‹é€²è¡Œ TensorRT è½‰æ›æ¸¬è©¦") 
print("3. å¦‚ä»æœ‰è­¦å‘Šï¼Œå¯ä»¥å®‰å…¨å¿½ç•¥ï¼Œä¸å½±éŸ¿æŽ¨è«–åŠŸèƒ½")
