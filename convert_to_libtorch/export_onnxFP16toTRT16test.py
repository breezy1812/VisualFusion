#!/usr/bin/env python3
"""
PyTorch to TensorRT Conversion Script (æ”¯æ´ FP16)
å¾ PyTorch æ¨¡å‹ç›´æ¥è½‰æ›ç‚º TensorRT å¼•æ“ï¼Œæ”¯æ´ FP16 æ¨¡å¼

è½‰æ›æµç¨‹ï¼š
- PyTorch æ¨¡å‹ï¼šå§‹çµ‚ä¿æŒ FP32 ç²¾åº¦ä»¥ç¢ºä¿æ•¸å€¼ç©©å®šæ€§
- ONNX æ¨¡å‹ï¼šå§‹çµ‚ä¿æŒ FP32 ç²¾åº¦ä»¥ç¢ºä¿ç›¸å®¹æ€§
- TensorRT å¼•æ“ï¼šå¯é¸æ“‡ FP16 ç²¾åº¦ä»¥æå‡æ¨ç†é€Ÿåº¦

åŸºæ–¼:
- /circ330/forgithub/VisualFusion_libtorch/convert_to_libtorch/model_jit æ¨¡å‹
- SemLA PyTorch æ¨¡å‹ç›´æ¥è½‰æ›

Usage:
    python export_onnxFP16toTRT16test.py --fp16  # TensorRT ä½¿ç”¨ FP16ï¼ŒPyTorch/ONNX ä¿æŒ FP32
    python export_onnxFP16toTRT16test.py         # å…¨ç¨‹ä½¿ç”¨ FP32 æ¨¡å¼
    python export_onnxFP16toTRT16test.py --fp16 --opset 12
"""

import tensorrt as trt
import numpy as np
import os
import argparse
import torch
import onnx
import tempfile
from pathlib import Path

# å°å…¥ SemLA æ¨¡å‹
from model_jit.SemLA import SemLA

class PyTorchToTensorRTConverter:
    def __init__(self):
        # å‰µå»º TensorRT loggerï¼Œä½¿ç”¨ WARNING ç­‰ç´šé¿å…éå¤šè¼¸å‡º
        self.logger = trt.Logger(trt.Logger.WARNING)

    def export_pytorch_to_onnx(self, use_fp16=False, opset_version=12, model_path="./reg.ckpt"):
        """
        å¾ PyTorch æ¨¡å‹å°å‡º ONNX æ¨¡å‹
        
        Args:
            use_fp16: æ˜¯å¦åœ¨ TensorRT ä¸­ä½¿ç”¨ FP16 ç²¾åº¦ï¼ˆONNX æ¨¡å‹ä¿æŒ FP32ï¼‰
            opset_version: ONNX opset ç‰ˆæœ¬
            model_path: PyTorch æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘
        
        Returns:
            str: è‡¨æ™‚ ONNX æª”æ¡ˆè·¯å¾‘
        """
        print("ğŸ¯ å¾ PyTorch æ¨¡å‹é–‹å§‹è½‰æ›...")
        
        # ä½¿ç”¨ CUDA ä¾†ç²å¾—æœ€ä½³æ€§èƒ½
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è¨­å‚™: {device}")

        # çµ±ä¸€ä½¿ç”¨ FP32 é€²è¡Œ PyTorch åˆ° ONNX çš„è½‰æ›ï¼Œç¢ºä¿æ•¸å€¼ç©©å®šæ€§
        fpMode = torch.float32
        if use_fp16:
            print("æ­£åœ¨è¼‰å…¥æ¨¡å‹ (FP32)ï¼Œç¨å¾Œåœ¨ TensorRT ä¸­å•Ÿç”¨ FP16...")
        else:
            print("æ­£åœ¨è¼‰å…¥æ¨¡å‹ (FP32)...")
            
        matcher = SemLA(device=device, fp=fpMode)
        matcher.load_state_dict(torch.load(model_path, map_location=device), strict=False)
        matcher = matcher.eval().to(device, dtype=fpMode)
        print("âœ… æ¨¡å‹å·²è¼‰å…¥ (FP32)")

        # ä½¿ç”¨èˆ‡é…ç½®æ–‡ä»¶ç›¸ç¬¦çš„å°ºå¯¸
        width = 320
        height = 240

        print(f"å»ºç«‹è¼¸å…¥å¼µé‡ï¼Œå°ºå¯¸: {height}x{width}, ç²¾åº¦: FP32")
        torch_input_1 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)
        torch_input_2 = torch.randn(1, 1, height, width).to(device, dtype=fpMode)

        # å‰µå»ºè‡¨æ™‚ ONNX æª”æ¡ˆ
        temp_onnx = tempfile.NamedTemporaryFile(suffix='.onnx', delete=False)
        onnx_path = temp_onnx.name
        temp_onnx.close()

        tensorrt_precision = "FP16" if use_fp16 else "FP32"
        print(f"è½‰æ›ç‚º FP32 ONNX æ¨¡å‹ï¼ˆTensorRT å°‡ä½¿ç”¨ {tensorrt_precision}ï¼‰...")
        print(f"ONNX OpSet ç‰ˆæœ¬: {opset_version}")

        try:
            torch.onnx.export(
                matcher,
                (torch_input_1, torch_input_2),
                onnx_path,
                verbose=False,
                opset_version=opset_version,
                input_names=["vi_img", "ir_img"],
                output_names=["mkpt0", "mkpt1", "leng1", "leng2"],
                do_constant_folding=True,
            )
            print(f"âœ… FP32 ONNX æ¨¡å‹è½‰æ›å®Œæˆ")
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ONNX è½‰æ›å¤±æ•—: {e}")
            if os.path.exists(onnx_path):
                os.unlink(onnx_path)
            return None

    def pytorch_to_tensorrt(self, model_path="./reg.ckpt", trt_path=None, use_fp16=False, opset_version=12, max_workspace_size=1<<30):
        """
        å¾ PyTorch æ¨¡å‹ç›´æ¥è½‰æ›ç‚º TensorRT å¼•æ“
        
        Args:
            model_path: PyTorch æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘
            trt_path: è¼¸å‡º TensorRT å¼•æ“è·¯å¾‘
            use_fp16: æ˜¯å¦ä½¿ç”¨ FP16 ç²¾åº¦
            opset_version: ONNX opset ç‰ˆæœ¬
            max_workspace_size: æœ€å¤§å·¥ä½œç©ºé–“å¤§å°
        
        Returns:
            bool: è½‰æ›æ˜¯å¦æˆåŠŸ
        """
        print("ğŸš€ PyTorch to TensorRT å®Œæ•´è½‰æ›æµç¨‹")
        print("=" * 50)
        
        # æ­¥é©Ÿ 1: å¾ PyTorch è½‰æ›ç‚º ONNX
        temp_onnx_path = self.export_pytorch_to_onnx(use_fp16, opset_version, model_path)
        if not temp_onnx_path:
            return False
            
        # æ­¥é©Ÿ 2: å¾ ONNX è½‰æ›ç‚º TensorRT
        precision_str = "fp16" if use_fp16 else "fp32"
        if trt_path is None:
            trt_path = f"./trt_semla_{precision_str}_op{opset_version}.engine"
            
        print(f"\nğŸ”„ è½‰æ› ONNX ç‚º TensorRT å¼•æ“...")
        success = self.convert_onnx_to_trt(
            onnx_path=temp_onnx_path,
            trt_path=trt_path,
            fp16_mode=use_fp16,
            max_workspace_size=max_workspace_size
        )
        
        # æ¸…ç†è‡¨æ™‚ ONNX æª”æ¡ˆ
        try:
            os.unlink(temp_onnx_path)
            print(f"ğŸ—‘ï¸  æ¸…ç†è‡¨æ™‚æª”æ¡ˆ: {temp_onnx_path}")
        except:
            pass
            
        return success

    def convert_onnx_to_trt(self, onnx_path, trt_path, max_batch_size=1, fp16_mode=True, max_workspace_size=1<<30):
        """
        å°‡ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT å¼•æ“

        Args:
            onnx_path: è¼¸å…¥ ONNX æ¨¡å‹è·¯å¾‘
            trt_path: è¼¸å‡º TensorRT å¼•æ“è·¯å¾‘
            max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
            fp16_mode: æ˜¯å¦å•Ÿç”¨ FP16 ç²¾åº¦
            max_workspace_size: æœ€å¤§å·¥ä½œç©ºé–“å¤§å° (bytes)
        """
        print(f"ğŸ”„ Converting ONNX to TensorRT...")
        print(f"ğŸ“ Input ONNX: {onnx_path}")
        print(f"ğŸ’¾ Output TRT: {trt_path}")

        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if not os.path.exists(onnx_path):
            raise FileNotFoundError(f"ONNX file not found: {onnx_path}")

        # å‰µå»º builder å’Œ network
        builder = trt.Builder(self.logger)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, self.logger)
        
        # è§£æ ONNX æ¨¡å‹
        print("ğŸ“– Parsing ONNX model...")
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print("âŒ Failed to parse ONNX model")
                for error in range(parser.num_errors):
                    print(f"  Error {error}: {parser.get_error(error)}")
                return False

        print("âœ… ONNX model parsed successfully")

        # é¡¯ç¤ºç¶²è·¯ä¿¡æ¯
        print(f"ğŸ“Š Network information:")
        print(f"  ğŸ”¢ Number of inputs: {network.num_inputs}")
        print(f"  ğŸ”¢ Number of outputs: {network.num_outputs}")

        for i in range(network.num_inputs):
            tensor = network.get_input(i)
            print(f"  ğŸ“¥ Input {i}: {tensor.name}")
            print(f"      Shape: {tensor.shape}")
            print(f"      Dtype: {tensor.dtype}")

        for i in range(network.num_outputs):
            tensor = network.get_output(i)
            print(f"  ğŸ“¤ Output {i}: {tensor.name}")
            print(f"      Shape: {tensor.shape}")
            print(f"      Dtype: {tensor.dtype}")

        # å‰µå»ºå»ºæ§‹é…ç½®
        config = builder.create_builder_config()
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)

        # å•Ÿç”¨ FP16 ç²¾åº¦ï¼ˆå¦‚æœæ”¯æŒä¸”è«‹æ±‚ï¼‰
        if fp16_mode and builder.platform_has_fast_fp16:
            print("ğŸš€ Enabling FP16 precision for faster inference")
            config.set_flag(trt.BuilderFlag.FP16)
        else:
            print("âš¡ Using FP32 precision for stability")

        # è¨­å®šå„ªåŒ–é…ç½®æ–‡ä»¶ï¼ˆé‡å°å›ºå®šè¼¸å…¥å½¢ç‹€ï¼‰
        profile = builder.create_optimization_profile()
        

        # åŸºæ–¼ SemLA æ¨¡å‹çš„å›ºå®šå½¢ç‹€è¨­å®š
        # è¼¸å…¥: vi_img (1, 1, 240, 320), ir_img (1, 1, 240, 320)
        input_shapes = [
            (1, 1, 240, 320),  # vi_img
            (1, 1, 240, 320)   # ir_img
        ]

        for i in range(network.num_inputs):
            tensor = network.get_input(i)
            shape = input_shapes[i]
            print(f"âš™ï¸  Setting optimization profile for {tensor.name}: {shape}")
            # ç¢ºä¿æ‰€æœ‰ min, opt, max éƒ½æ˜¯å›ºå®šå½¢ç‹€ï¼Œé¿å…å‹•æ…‹å°ºå¯¸å•é¡Œ
            profile.set_shape(tensor.name, shape, shape, shape)

        # ç§»é™¤ is_valid() æª¢æŸ¥ï¼Œå› ç‚º TensorRT 10.x ç‰ˆæœ¬æ²’æœ‰é€™å€‹æ–¹æ³•
        config.add_optimization_profile(profile)

        # å»ºæ§‹ TensorRT å¼•æ“
        print("ğŸ”„ Building TensorRT engine... (this may take several minutes)")
        print("   ğŸ’­ Please be patient, optimizing network for your hardware...")

        serialized_engine = builder.build_serialized_network(network, config)

        if serialized_engine is None:
            print("âŒ Failed to build TensorRT engine")
            return False

        # å„²å­˜å¼•æ“åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(trt_path), exist_ok=True)
        with open(trt_path, 'wb') as f:
            f.write(serialized_engine)

        print(f"âœ… TensorRT engine saved successfully!")
        print(f"ğŸ’¾ Engine file: {trt_path}")
        print(f"ğŸ“ File size: {os.path.getsize(trt_path) / (1024*1024):.2f} MB")

        # é©—è­‰å¼•æ“
        return self.validate_engine(trt_path)

    def validate_engine(self, trt_path):
        """é©—è­‰å‰µå»ºçš„ TensorRT å¼•æ“"""
        print("ğŸ” Validating TensorRT engine...")

        try:
            # è¼‰å…¥å¼•æ“
            runtime = trt.Runtime(self.logger)
            with open(trt_path, 'rb') as f:
                engine_data = f.read()

            engine = runtime.deserialize_cuda_engine(engine_data)
            if not engine:
                print("âŒ Failed to load created engine")
                return False

            context = engine.create_execution_context()
            if not context:
                print("âŒ Failed to create execution context")
                return False

            print("ğŸ“‹ Engine validation results:")
            print(f"  ğŸ”¢ Number of bindings: {engine.num_bindings}")

            for i in range(engine.num_bindings):
                name = engine.get_binding_name(i)
                shape = engine.get_binding_shape(i)
                dtype = engine.get_binding_dtype(i)
                is_input = engine.binding_is_input(i)
                binding_type = "Input" if is_input else "Output"

                print(f"  {'ğŸ“¥' if is_input else 'ğŸ“¤'} {binding_type} {i}: {name}")
                print(f"      Shape: {shape}")
                print(f"      Dtype: {dtype}")

            print("âœ… Engine validation passed!")
            return True

        except Exception as e:
            print(f"âŒ Engine validation failed: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description='Convert PyTorch/ONNX model to TensorRT engine')
    parser.add_argument('--model', type=str,
                       default='./reg.ckpt',
                       help='Path to PyTorch model checkpoint (default: ./reg.ckpt)')
    parser.add_argument('--onnx', type=str, default=None,
                       help='Path to input ONNX model (if provided, skip PyTorch conversion)')
    parser.add_argument('--trt', type=str, default=None,
                       help='Path to output TensorRT engine (auto-generated if not provided)')
    parser.add_argument('--fp16', action='store_true', default=False,
                       help='Enable FP16 precision in TensorRT (PyTorch and ONNX remain FP32)')
    parser.add_argument('--opset', type=int, default=12,
                       help='ONNX opset version (default: 12)')
    parser.add_argument('--workspace-size', type=int, default=1024,
                       help='Max workspace size in MB (default: 1024)')

    args = parser.parse_args()

    print("ğŸ¯ PyTorch/ONNX to TensorRT Conversion Tool")
    print("=" * 60)
    print("ğŸ“‹ Configuration:")
    
    # å‰µå»ºè½‰æ›å™¨
    converter = PyTorchToTensorRTConverter()
    
    if args.onnx:
        # å¾ ONNX è½‰æ›æ¨¡å¼
        print(f"  ğŸ“ ONNX model: {args.onnx}")
        print(f"  ğŸ’¾ TRT engine: {args.trt}")
        print(f"  ğŸš€ FP16 mode: {args.fp16}")
        print(f"  ğŸ’¾ Workspace: {args.workspace_size} MB")
        print("=" * 60)
        
        success = converter.convert_onnx_to_trt(
            onnx_path=args.onnx,
            trt_path=args.trt,
            fp16_mode=args.fp16,
            max_workspace_size=args.workspace_size * 1024 * 1024
        )
    else:
        # å¾ PyTorch å®Œæ•´è½‰æ›æ¨¡å¼
        precision_str = "fp16" if args.fp16 else "fp32"
        if args.trt is None:
            args.trt = f"./trt_semla_{precision_str}_op{args.opset}.engine"
            
        print(f"  ğŸ§  PyTorch model: {args.model}")
        print(f"  ğŸ’¾ TRT engine: {args.trt}")
        print(f"  ğŸš€ FP16 mode: {args.fp16}")
        print(f"  ğŸ”§ ONNX OpSet: {args.opset}")
        print(f"  ğŸ’¾ Workspace: {args.workspace_size} MB")
        print("=" * 60)
        
        success = converter.pytorch_to_tensorrt(
            model_path=args.model,
            trt_path=args.trt,
            use_fp16=args.fp16,
            opset_version=args.opset,
            max_workspace_size=args.workspace_size * 1024 * 1024
        )

    if success:
        print("\nğŸ‰ Conversion completed successfully!")
        print(f"ğŸ“Œ TensorRT engine: {args.trt}")
        print("ğŸ”§ Update your configuration files to use this new engine.")
    else:
        print("\nğŸ’¥ Conversion failed!")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())
