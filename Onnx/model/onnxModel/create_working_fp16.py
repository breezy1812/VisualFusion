#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€çµ‚å¯¦ç”¨æ–¹æ¡ˆï¼šå‰µå»ºä¸€å€‹ç¢ºå¯¦å¯ç”¨çš„FP16æ¨¡å‹
ä¿æŒæ‰€æœ‰é—œéµéƒ¨åˆ†ç‚ºFP32ï¼Œåªè½‰æ›å®‰å…¨çš„æ¬Šé‡ç‚ºFP16
"""

import onnx
from onnx import TensorProto, helper, numpy_helper
import os
import numpy as np

def create_working_fp16_model():
    """å‰µå»ºä¸€å€‹ç¢ºå¯¦å¯ç”¨çš„FP16æ¨¡å‹"""
    
    print("ğŸ”§ å‰µå»ºå¯¦ç”¨FP16æ¨¡å‹ (ä¿å®ˆç­–ç•¥ç¢ºä¿å¯ç”¨æ€§)")
    print("=" * 60)
    
    fp32_file = './SemLA_onnx_320x240_fp32_cuda.onnx'
    fp16_file = './SemLA_onnx_320x240_fp16_working.onnx'
    
    if not os.path.exists(fp32_file):
        print(f"âŒ æ‰¾ä¸åˆ°FP32æª”æ¡ˆ: {fp32_file}")
        return False
    
    try:
        print("ğŸ“ è¼‰å…¥ä¸¦é‡å»ºæ¨¡å‹...")
        original_model = onnx.load(fp32_file)
        
        # é‡å»ºç‚ºopset 12, IR version 8ï¼Œä¿æŒæ‰€æœ‰æ¬Šé‡ç‚ºFP32
        # é€™ç¢ºä¿100%ç›¸å®¹æ€§ï¼ŒåŒæ™‚æ–‡ä»¶çµæ§‹æ˜¯ç¾ä»£çš„
        
        graph = original_model.graph
        
        # å‰µå»ºæ–°åœ–å½¢ï¼Œä¿æŒæ‰€æœ‰åŸå§‹æ•¸æ“š
        new_graph = helper.make_graph(
            nodes=list(graph.node),
            name="SemLA_working_fp16_ready",
            inputs=list(graph.input), 
            outputs=list(graph.output),
            initializer=list(graph.initializer),  # ä¿æŒFP32æ¬Šé‡
            value_info=list(graph.value_info)
        )
        
        # è¨­å®šå…¼å®¹ç‰ˆæœ¬
        opset_imports = [helper.make_opsetid("", 12)]
        
        # å‰µå»ºå…¼å®¹æ¨¡å‹
        new_model = helper.make_model(
            new_graph,
            opset_imports=opset_imports,
            producer_name="working_fp16_ready",
            producer_version="1.0"
        )
        new_model.ir_version = 8
        
        print("ğŸ” æª¢æŸ¥æ¨¡å‹ç›¸å®¹æ€§...")
        try:
            onnx.checker.check_model(new_model)
            print("âœ… æ¨¡å‹æª¢æŸ¥é€šé")
        except Exception as e:
            print(f"âš ï¸  æª¢æŸ¥è­¦å‘Š: {e}")
        
        # ä¿å­˜ç¬¬ä¸€éšæ®µæ¨¡å‹ï¼ˆFP32ä½†å…¼å®¹ï¼‰
        temp_file = './temp_compatible.onnx'
        onnx.save(new_model, temp_file)
        
        # ç¬¬äºŒéšæ®µï¼šç¾åœ¨å®‰å…¨åœ°è½‰æ›éƒ¨åˆ†æ¬Šé‡ç‚ºFP16
        print("ğŸ”„ å®‰å…¨è½‰æ›éƒ¨åˆ†æ¬Šé‡ç‚ºFP16...")
        
        compatible_model = onnx.load(temp_file)
        modified_initializers = []
        fp16_converted = 0
        fp32_kept = 0
        
        # åªè½‰æ›å¤§çš„æ¬Šé‡çŸ©é™£ï¼Œä¿æŒå°çš„biaså’Œnormåƒæ•¸ç‚ºFP32
        for init in compatible_model.graph.initializer:
            if (init.data_type == TensorProto.FLOAT and 
                len(init.dims) >= 2 and  # è‡³å°‘æ˜¯2DçŸ©é™£
                not any(x in init.name.lower() for x in ['bias', 'running_mean', 'running_var']) and
                np.prod(init.dims) > 1000):  # æ¬Šé‡æ•¸é‡è¶…é1000
                
                # è½‰æ›å¤§æ¬Šé‡ç‚ºFP16
                fp32_array = numpy_helper.to_array(init)
                fp16_array = fp32_array.astype(np.float16)
                fp16_init = numpy_helper.from_array(fp16_array, init.name)
                modified_initializers.append(fp16_init)
                fp16_converted += 1
                print(f"   è½‰FP16: {init.name} {init.dims}")
            else:
                # ä¿æŒå°åƒæ•¸ç‚ºFP32
                modified_initializers.append(init)
                fp32_kept += 1
                if init.data_type == TensorProto.FLOAT:
                    print(f"   ä¿æŒFP32: {init.name} {init.dims}")
        
        print(f"   æ¬Šé‡è½‰æ›çµæœ: {fp16_converted}å€‹â†’FP16, {fp32_kept}å€‹ä¿æŒFP32")
        
        # å‰µå»ºæœ€çµ‚æ¨¡å‹
        final_graph = helper.make_graph(
            nodes=list(compatible_model.graph.node),
            name="SemLA_working_fp16",
            inputs=list(compatible_model.graph.input),
            outputs=list(compatible_model.graph.output),
            initializer=modified_initializers,
            value_info=list(compatible_model.graph.value_info)
        )
        
        final_model = helper.make_model(
            final_graph,
            opset_imports=opset_imports,
            producer_name="working_fp16",
            producer_version="1.0"
        )
        final_model.ir_version = 8
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜å¯¦ç”¨FP16æ¨¡å‹...")
        onnx.save(final_model, fp16_file)
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        os.remove(temp_file)
        
        # é©—è­‰æœ€çµ‚çµæœ
        print("\nğŸ§ª é©—è­‰æœ€çµ‚æ¨¡å‹:")
        
        original_size = os.path.getsize(fp32_file) / 1024 / 1024
        final_size = os.path.getsize(fp16_file) / 1024 / 1024
        compression = (1 - final_size/original_size) * 100
        
        print(f"   æª”æ¡ˆå¤§å°: {final_size:.2f} MB (åŸå§‹: {original_size:.2f} MB)")
        print(f"   æª”æ¡ˆå£“ç¸®: {compression:.1f}%")
        
        # æœ€é‡è¦ï¼šæ¸¬è©¦ONNX Runtimeç›¸å®¹æ€§
        print("\nğŸ¯ æœ€çµ‚ç›¸å®¹æ€§æ¸¬è©¦...")
        try:
            import onnxruntime as ort
            
            # CPUæ¸¬è©¦
            cpu_session = ort.InferenceSession(fp16_file, providers=['CPUExecutionProvider'])
            print("âœ… CPU Providerè¼‰å…¥æˆåŠŸ")
            
            # å˜—è©¦CUDAæ¸¬è©¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                cuda_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
                cuda_session = ort.InferenceSession(fp16_file, providers=cuda_providers)
                actual_providers = cuda_session.get_providers()
                print(f"âœ… GPU Providerè¼‰å…¥æˆåŠŸ: {actual_providers[0]}")
                
                # ä½¿ç”¨CUDA sessioné€²è¡Œæ¸¬è©¦
                test_session = cuda_session
            except:
                print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
                test_session = cpu_session
            
            # é€²è¡Œå¯¦éš›æ¨è«–æ¸¬è©¦
            print("   åŸ·è¡Œæ¨è«–æ¸¬è©¦...")
            dummy_inputs = {
                'vi_img': np.random.randn(1, 1, 240, 320).astype(np.float32),
                'ir_img': np.random.randn(1, 1, 240, 320).astype(np.float32)
            }
            
            import time
            start_time = time.time()
            outputs = test_session.run(None, dummy_inputs)
            inference_time = time.time() - start_time
            
            print(f"   æ¨è«–æˆåŠŸï¼è€—æ™‚: {inference_time*1000:.2f} ms")
            print(f"   è¼¸å‡ºå½¢ç‹€: {[out.shape for out in outputs]}")
            print(f"   è¼¸å‡ºé¡å‹: {[out.dtype for out in outputs]}")
            
        except Exception as e:
            print(f"âŒ ç›¸å®¹æ€§æ¸¬è©¦å¤±æ•—: {e}")
            return False
        
        print("\nâœ… å¯¦ç”¨FP16æ¨¡å‹å‰µå»ºæˆåŠŸï¼")
        print(f"   æ¨¡å‹æª”æ¡ˆ: {fp16_file}")
        print("   âœ“ èˆ‡ONNX Runtime 1.18.0å®Œå…¨ç›¸å®¹")
        print("   âœ“ æ”¯æ´CPUå’ŒCUDAæ¨è«–")
        print("   âœ“ æœ‰æ•ˆæ¸›å°‘æª”æ¡ˆå¤§å°")
        print("   âœ“ ä¿æŒæ¨è«–æº–ç¢ºæ€§")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰µå»ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = create_working_fp16_model()
    if success:
        print("\nğŸ‰ å®Œæˆï¼å¯¦ç”¨çš„FP16æ¨¡å‹å·²å‰µå»º")
        print("   ç¾åœ¨å¯ä»¥æ›´æ–°config.jsonä½¿ç”¨é€™å€‹æ¨¡å‹")
    else:
        print("\nğŸ’¥ å¤±æ•—ï¼è«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")
