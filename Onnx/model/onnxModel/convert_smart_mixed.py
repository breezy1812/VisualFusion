#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ··åˆç²¾åº¦ONNXè½‰æ›ï¼šä¿æŒè¼¸å…¥è¼¸å‡ºç‚ºFP32ï¼Œå®‰å…¨åœ°è½‰æ›ä¸­é–“å±¤æ¬Šé‡ç‚ºFP16
"""

import onnx
from onnx import TensorProto, helper, numpy_helper
import os
import numpy as np

def create_smart_mixed_precision():
    """å‰µå»ºæ™ºèƒ½æ··åˆç²¾åº¦æ¨¡å‹"""
    
    print("ğŸ”§ æ™ºèƒ½æ··åˆç²¾åº¦è½‰æ› (FP32è¼¸å…¥/è¼¸å‡º + FP16æ¬Šé‡)")
    print("=" * 60)
    
    fp32_file = './SemLA_onnx_320x240_fp32_cuda.onnx'
    fp16_file = './SemLA_onnx_320x240_fp16_smart.onnx'
    
    if not os.path.exists(fp32_file):
        print(f"âŒ æ‰¾ä¸åˆ°FP32æª”æ¡ˆ: {fp32_file}")
        return False
    
    try:
        # è¼‰å…¥åŸå§‹FP32æ¨¡å‹
        print("ğŸ“ è¼‰å…¥FP32æ¨¡å‹...")
        model = onnx.load(fp32_file)
        graph = model.graph
        
        print(f"   ç¯€é»æ•¸: {len(graph.node)}")
        print(f"   æ¬Šé‡æ•¸: {len(graph.initializer)}")
        
        # åˆ†æè¼¸å…¥è¼¸å‡ºå¼µé‡åç¨±
        input_names = set(inp.name for inp in graph.input)
        output_names = set(out.name for out in graph.output)
        print(f"   è¼¸å…¥: {input_names}")
        print(f"   è¼¸å‡º: {output_names}")
        
        # åˆ†æç¯€é»ï¼Œæ‰¾å‡ºå“ªäº›æ¬Šé‡å¯ä»¥å®‰å…¨è½‰ç‚ºFP16
        print("ğŸ” åˆ†æç¯€é»é¡å‹ç›¸å®¹æ€§...")
        
        # æ”¶é›†æ‰€æœ‰ç¯€é»çš„è¼¸å…¥è¼¸å‡º
        node_inputs = set()
        node_outputs = set()
        problematic_nodes = []
        
        for node in graph.node:
            for inp in node.input:
                node_inputs.add(inp)
            for out in node.output:
                node_outputs.add(out)
            
            # æª¢æŸ¥å¯èƒ½æœ‰å•é¡Œçš„ç¯€é»é¡å‹
            if node.op_type in ['Conv', 'MatMul', 'BatchNormalization', 'Add']:
                problematic_nodes.append((node.name, node.op_type, node.input, node.output))
        
        print(f"   ç™¼ç¾ {len(problematic_nodes)} å€‹éœ€è¦ç‰¹åˆ¥è™•ç†çš„ç¯€é»")
        
        # è½‰æ›ç­–ç•¥ï¼š
        # 1. ä¿æŒæ‰€æœ‰è¼¸å…¥/è¼¸å‡ºç›¸é—œçš„æ¬Šé‡ç‚ºFP32
        # 2. è½‰æ›ä¸­é–“å±¤æ¬Šé‡ç‚ºFP16
        # 3. æ·»åŠ å¿…è¦çš„Castç¯€é»
        
        print("ğŸ”„ æ™ºèƒ½æ¬Šé‡è½‰æ›...")
        new_initializers = []
        new_nodes = list(graph.node)
        fp32_kept = 0
        fp16_converted = 0
        cast_nodes_added = 0
        
        # æ”¶é›†éœ€è¦ä¿æŒFP32çš„æ¬Šé‡åç¨±
        critical_weights = set()
        
        # æª¢æŸ¥æ¯å€‹åˆå§‹åŒ–å™¨
        for initializer in graph.initializer:
            weight_name = initializer.name
            is_critical = False
            
            # æª¢æŸ¥é€™å€‹æ¬Šé‡æ˜¯å¦è¢«è¼¸å…¥/è¼¸å‡ºç›¸é—œçš„ç¯€é»ä½¿ç”¨
            for node_name, op_type, inputs, outputs in problematic_nodes:
                if weight_name in inputs:
                    # æª¢æŸ¥é€™å€‹ç¯€é»çš„å…¶ä»–è¼¸å…¥æ˜¯å¦ä¾†è‡ªè¼¸å…¥æˆ–è¼¸å‡º
                    for inp in inputs:
                        if inp in input_names or inp in output_names:
                            is_critical = True
                            critical_weights.add(weight_name)
                            break
                    
                    # æª¢æŸ¥ç¯€é»è¼¸å‡ºæ˜¯å¦ç›´æ¥é€£åˆ°è¼¸å‡º
                    for out in outputs:
                        if out in output_names:
                            is_critical = True
                            critical_weights.add(weight_name)
                            break
            
            if initializer.data_type == TensorProto.FLOAT:
                if is_critical or weight_name.endswith('bias') or 'norm' in weight_name.lower():
                    # ä¿æŒé—œéµæ¬Šé‡ç‚ºFP32
                    new_initializers.append(initializer)
                    fp32_kept += 1
                    if is_critical:
                        print(f"   ä¿æŒFP32 (é—œéµ): {weight_name}")
                else:
                    # è½‰æ›ç‚ºFP16
                    fp32_weights = numpy_helper.to_array(initializer)
                    fp16_weights = fp32_weights.astype(np.float16)
                    new_initializer = numpy_helper.from_array(fp16_weights, weight_name)
                    new_initializers.append(new_initializer)
                    fp16_converted += 1
                    
                    # å°æ–¼Convå’ŒMatMulç¯€é»ï¼Œæ·»åŠ Castç¯€é»
                    for i, node in enumerate(new_nodes):
                        if weight_name in node.input and node.op_type in ['Conv', 'MatMul']:
                            # å‰µå»ºCastç¯€é»å°‡FP16æ¬Šé‡è½‰å›FP32
                            cast_output_name = f"{weight_name}_casted_fp32"
                            cast_node = helper.make_node(
                                'Cast',
                                inputs=[weight_name],
                                outputs=[cast_output_name],
                                to=TensorProto.FLOAT,
                                name=f"cast_{weight_name}_to_fp32"
                            )
                            
                            # ä¿®æ”¹åŸç¯€é»ä½¿ç”¨Castå¾Œçš„æ¬Šé‡
                            new_inputs = [cast_output_name if inp == weight_name else inp for inp in node.input]
                            new_nodes[i] = helper.make_node(
                                node.op_type,
                                inputs=new_inputs,
                                outputs=node.output,
                                name=node.name,
                                **{attr.name: attr for attr in node.attribute}
                            )
                            
                            new_nodes.insert(i, cast_node)
                            cast_nodes_added += 1
                            print(f"   è½‰FP16+Cast: {weight_name}")
                            break
                    else:
                        print(f"   è½‰FP16: {weight_name}")
            else:
                new_initializers.append(initializer)
        
        print(f"   è½‰æ›çµæœ: FP32ä¿æŒ={fp32_kept}, FP16è½‰æ›={fp16_converted}, Castç¯€é»={cast_nodes_added}")
        
        # é‡å»ºåœ–å½¢
        print("ğŸ—ï¸  é‡å»ºæ··åˆç²¾åº¦åœ–å½¢...")
        new_graph = helper.make_graph(
            nodes=new_nodes,
            name="SemLA_smart_mixed_precision",
            inputs=list(graph.input),
            outputs=list(graph.output),
            initializer=new_initializers,
            value_info=list(graph.value_info)
        )
        
        # å‰µå»ºæ–°æ¨¡å‹
        opset_imports = [helper.make_opsetid("", 12)]
        new_model = helper.make_model(
            new_graph,
            opset_imports=opset_imports,
            producer_name="smart_mixed_precision",
            producer_version="1.0"
        )
        new_model.ir_version = 8
        
        # æª¢æŸ¥æ¨¡å‹
        print("ğŸ” æª¢æŸ¥æ¨¡å‹...")
        try:
            onnx.checker.check_model(new_model)
            print("âœ… æ¨¡å‹æª¢æŸ¥é€šé")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹æª¢æŸ¥è­¦å‘Š: {e}")
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ™ºèƒ½æ··åˆç²¾åº¦æ¨¡å‹...")
        onnx.save(new_model, fp16_file)
        
        # é©—è­‰çµæœ
        print("\nğŸ§ª é©—è­‰æ··åˆç²¾åº¦è½‰æ›:")
        test_model = onnx.load(fp16_file)
        
        # çµ±è¨ˆæ¬Šé‡é¡å‹
        real_fp16_count = 0
        real_fp32_count = 0
        
        for init in test_model.graph.initializer:
            if init.data_type == TensorProto.FLOAT16:
                real_fp16_count += 1
            elif init.data_type == TensorProto.FLOAT:
                real_fp32_count += 1
        
        original_size = os.path.getsize(fp32_file) / 1024 / 1024
        new_size = os.path.getsize(fp16_file) / 1024 / 1024
        
        print(f"   æª”æ¡ˆå¤§å°: {new_size:.2f} MB (åŸå§‹: {original_size:.2f} MB)")
        print(f"   å£“ç¸®ç‡: {(1 - new_size/original_size)*100:.1f}%")
        print(f"   æ¬Šé‡çµ±è¨ˆ: FP16={real_fp16_count}, FP32={real_fp32_count}")
        print(f"   ç¯€é»ç¸½æ•¸: {len(test_model.graph.node)} (æ–°å¢Cast: {cast_nodes_added})")
        
        # æ¸¬è©¦ONNX Runtimeè¼‰å…¥
        print("\nğŸ¯ æ¸¬è©¦ONNX Runtimeè¼‰å…¥...")
        try:
            import onnxruntime as ort
            session = ort.InferenceSession(fp16_file, providers=['CPUExecutionProvider'])
            print("âœ… ONNX RuntimeæˆåŠŸè¼‰å…¥æ··åˆç²¾åº¦æ¨¡å‹")
            
            # ç°¡å–®æ¨è«–æ¸¬è©¦
            print("   é€²è¡Œæ¸¬è©¦æ¨è«–...")
            dummy_input = {
                'vi_img': np.random.randn(1, 1, 240, 320).astype(np.float32),
                'ir_img': np.random.randn(1, 1, 240, 320).astype(np.float32)
            }
            outputs = session.run(None, dummy_input)
            print(f"   æ¨è«–æˆåŠŸï¼Œè¼¸å‡ºå½¢ç‹€: {[out.shape for out in outputs]}")
            
        except Exception as e:
            print(f"âŒ ONNX Runtimeæ¸¬è©¦å¤±æ•—: {e}")
            return False
        
        print("\nâœ… æ™ºèƒ½æ··åˆç²¾åº¦æ¨¡å‹å‰µå»ºå®Œæˆï¼")
        print(f"   è¼¸å‡ºæª”æ¡ˆ: {fp16_file}")
        return True
        
    except Exception as e:
        print(f"âŒ æ··åˆç²¾åº¦è½‰æ›å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = create_smart_mixed_precision()
    if success:
        print("\nğŸ‰ æˆåŠŸï¼æ™ºèƒ½æ··åˆç²¾åº¦æ¨¡å‹å·²å‰µå»º")
        print("   æ—¢æœ‰FP16æ¬Šé‡å„ªåŒ–ï¼Œåˆèƒ½æ­£ç¢ºæ¨è«–")
    else:
        print("\nğŸ’¥ å¤±æ•—ï¼è«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")
