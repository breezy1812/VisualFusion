#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿä¿®å¾© FP16 æ¨¡å‹é¡å‹ä¸åŒ¹é…å•é¡Œ
å°ˆé–€è§£æ±º Concat ç¯€é»çš„ tensor(float16) vs tensor(float) éŒ¯èª¤
"""

import onnx
from onnx import TensorProto, helper
import os
import sys

def quick_fix_fp16_model():
    """å¿«é€Ÿä¿®å¾©FP16æ¨¡å‹çš„é¡å‹ä¸åŒ¹é…å•é¡Œ"""
    
    print("ğŸ”§ å¿«é€Ÿä¿®å¾© FP16 æ¨¡å‹é¡å‹ä¸åŒ¹é…")
    print("=" * 50)
    
    # æª”æ¡ˆè·¯å¾‘
    fp32_file = './SemLA_onnx_320x240_fp32_cuda.onnx'
    fp16_file = './SemLA_onnx_320x240_fp16_cuda.onnx'
    
    if not os.path.exists(fp32_file):
        print(f"âŒ æ‰¾ä¸åˆ° FP32 æª”æ¡ˆ: {fp32_file}")
        return False
    
    try:
        # è¼‰å…¥FP32æ¨¡å‹
        print("ğŸ“ è¼‰å…¥ FP32 æ¨¡å‹...")
        model = onnx.load(fp32_file)
        graph = model.graph
        
        # å‚™ä»½èˆŠçš„FP16æ¨¡å‹
        if os.path.exists(fp16_file):
            backup_file = fp16_file.replace('.onnx', '_old.onnx')
            os.rename(fp16_file, backup_file)
            print(f"ğŸ’¾ å‚™ä»½èˆŠæ¨¡å‹: {backup_file}")
        
        # æ­¥é©Ÿ1ï¼šå‰µå»ºæ··åˆç²¾åº¦æ¨¡å‹
        print("ğŸ”„ å‰µå»ºæ··åˆç²¾åº¦æ¨¡å‹...")
        
        # è¤‡è£½æ‰€æœ‰ç¯€é»ï¼Œä½†ä¿®æ”¹é—œéµç¯€é»çš„è¡Œç‚º
        new_nodes = []
        new_value_infos = []
        new_initializers = []
        
        # è™•ç†åˆå§‹åŒ–å™¨ - æ¬Šé‡è½‰ç‚ºFP16
        for init in graph.initializer:
            if init.data_type == TensorProto.FLOAT and init.name not in ['input', 'output']:
                # è½‰æ›æ¬Šé‡ç‚ºFP16ä»¥ç¯€çœç©ºé–“
                new_init = onnx.TensorProto()
                new_init.CopyFrom(init)
                
                # ç°¡å–®çš„FP16è½‰æ›
                import numpy as np
                if init.raw_data:
                    # å¾raw_dataè½‰æ›
                    float32_data = np.frombuffer(init.raw_data, dtype=np.float32)
                    float16_data = float32_data.astype(np.float16)
                    new_init.raw_data = float16_data.tobytes()
                    new_init.data_type = TensorProto.FLOAT16
                elif init.float_data:
                    # å¾float_dataè½‰æ›
                    float32_data = np.array(init.float_data)
                    float16_data = float32_data.astype(np.float16)
                    # æ¸…é™¤èˆŠè³‡æ–™
                    new_init.float_data[:] = []
                    # è½‰æ›ç‚ºint32_dataæ ¼å¼
                    int_data = [int(x.view('uint16')) for x in float16_data.flat]
                    new_init.int32_data[:] = int_data
                    new_init.data_type = TensorProto.FLOAT16
                
                new_initializers.append(new_init)
            else:
                new_initializers.append(init)
        
        # è™•ç†ç¯€é» - ä¿æŒé—œéµç¯€é»ç‚ºFP32
        critical_ops = ['Concat', 'ReduceMean', 'Pow', 'Cast', 'Shape', 'Gather', 'ConstantOfShape']
        
        for node in graph.node:
            new_nodes.append(node)  # ä¿æŒåŸå§‹ç¯€é»ä¸è®Š
        
        # è™•ç†value_info - æ™ºèƒ½é¡å‹åˆ†é…
        input_output_names = {inp.name for inp in graph.input} | {out.name for out in graph.output}
        
        for vi in graph.value_info:
            new_vi = onnx.ValueInfoProto()
            new_vi.CopyFrom(vi)
            
            # æ ¹æ“šåç¨±æ¨¡å¼æ±ºå®šé¡å‹
            if vi.name in input_output_names:
                # è¼¸å…¥è¼¸å‡ºä¿æŒFLOAT32
                new_vi.type.tensor_type.elem_type = TensorProto.FLOAT
            elif any(pattern in vi.name.lower() for pattern in [
                'concat', 'reducemean', 'pow', 'cast', 'shape', 'gather', 
                'output_cast', 'input_cast', '/concat_output_cast_0'
            ]):
                # é—œéµæ“ä½œä¿æŒFLOAT32
                new_vi.type.tensor_type.elem_type = TensorProto.FLOAT
            elif 'constantofshape' in vi.name.lower() or 'shape' in vi.name.lower():
                # å½¢ç‹€ç›¸é—œæ“ä½œä¿æŒINT64
                if new_vi.type.tensor_type.elem_type in [TensorProto.FLOAT, TensorProto.FLOAT16]:
                    new_vi.type.tensor_type.elem_type = TensorProto.INT64
            else:
                # å…¶ä»–ä¸­é–“å¼µé‡ä½¿ç”¨FLOAT16ä»¥ç¯€çœè¨˜æ†¶é«”
                if new_vi.type.tensor_type.elem_type == TensorProto.FLOAT:
                    new_vi.type.tensor_type.elem_type = TensorProto.FLOAT16
            
            new_value_infos.append(new_vi)
        
        # å‰µå»ºæ–°çš„åœ–
        new_graph = helper.make_graph(
            nodes=new_nodes,
            name=graph.name,
            inputs=graph.input,  # ä¿æŒåŸå§‹è¼¸å…¥
            outputs=graph.output,  # ä¿æŒåŸå§‹è¼¸å‡º
            initializer=new_initializers,
            value_info=new_value_infos
        )
        
        # å‰µå»ºæ–°æ¨¡å‹
        new_model = helper.make_model(new_graph)
        new_model.ir_version = model.ir_version
        new_model.producer_name = model.producer_name
        new_model.producer_version = model.producer_version
        new_model.domain = model.domain
        new_model.model_version = model.model_version
        
        # è¤‡è£½opsetè³‡è¨Šï¼Œä¸¦ç¢ºä¿ç‰ˆæœ¬ç›¸å®¹
        for opset in model.opset_import:
            new_opset = new_model.opset_import.add()
            new_opset.CopyFrom(opset)
            if new_opset.version > 21:
                new_opset.version = 21
        
        # è¤‡è£½metadata
        if model.metadata_props:
            new_model.metadata_props.extend(model.metadata_props)
        
        print("âœ… æ··åˆç²¾åº¦æ¨¡å‹å‰µå»ºå®Œæˆ")
        
        # é©—è­‰æ¨¡å‹
        print("ğŸ” é©—è­‰æ¨¡å‹...")
        try:
            onnx.checker.check_model(new_model)
            print("   âœ… æ¨¡å‹é©—è­‰é€šé")
        except Exception as e:
            print(f"   âš ï¸  é©—è­‰è­¦å‘Š: {e}")
            print("   ç¹¼çºŒä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ··åˆç²¾åº¦æ¨¡å‹: {fp16_file}")
        onnx.save(new_model, fp16_file)
        
        # çµ±è¨ˆè³‡è¨Š
        fp32_size = os.path.getsize(fp32_file) / 1024 / 1024
        fp16_size = os.path.getsize(fp16_file) / 1024 / 1024
        
        print("\nğŸ‰ æ··åˆç²¾åº¦æ¨¡å‹å‰µå»ºå®Œæˆï¼")
        print("=" * 50)
        print(f"FP32 æ¨¡å‹: {fp32_size:.2f} MB")
        print(f"æ··åˆç²¾åº¦æ¨¡å‹: {fp16_size:.2f} MB")
        print(f"ç¯€çœç©ºé–“: {(1-fp16_size/fp32_size)*100:.1f}%")
        print("\nâœ… æ¨¡å‹ç‰¹æ€§:")
        print("  - è¼¸å…¥è¼¸å‡º: FLOAT32 (ç›¸å®¹æ€§)")
        print("  - æ¬Šé‡åƒæ•¸: FLOAT16 (ç¯€çœè¨˜æ†¶é«”)")
        print("  - é—œéµç¯€é»: FLOAT32 (é¿å…é¡å‹éŒ¯èª¤)")
        print("  - ä¸­é–“å¼µé‡: FLOAT16 (åŠ é€Ÿè¨ˆç®—)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¾©å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_mixed_precision_model():
    """é©—è­‰æ··åˆç²¾åº¦æ¨¡å‹çš„é¡å‹æ­£ç¢ºæ€§"""
    
    fp16_file = './SemLA_onnx_320x240_fp16_cuda.onnx'
    
    if not os.path.exists(fp16_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {fp16_file}")
        return False
    
    print("\nğŸ” é©—è­‰æ··åˆç²¾åº¦æ¨¡å‹...")
    print("-" * 40)
    
    try:
        model = onnx.load(fp16_file)
        graph = model.graph
        
        # çµ±è¨ˆä¸åŒé¡å‹çš„å¼µé‡
        type_counts = {
            'FLOAT': 0,
            'FLOAT16': 0,
            'INT64': 0,
            'OTHER': 0
        }
        
        # æª¢æŸ¥value_info
        for vi in graph.value_info:
            elem_type = vi.type.tensor_type.elem_type
            if elem_type == TensorProto.FLOAT:
                type_counts['FLOAT'] += 1
            elif elem_type == TensorProto.FLOAT16:
                type_counts['FLOAT16'] += 1
            elif elem_type == TensorProto.INT64:
                type_counts['INT64'] += 1
            else:
                type_counts['OTHER'] += 1
        
        # æª¢æŸ¥åˆå§‹åŒ–å™¨
        init_fp16 = sum(1 for init in graph.initializer if init.data_type == TensorProto.FLOAT16)
        init_fp32 = sum(1 for init in graph.initializer if init.data_type == TensorProto.FLOAT)
        
        print(f"ğŸ“Š å¼µé‡é¡å‹çµ±è¨ˆ:")
        print(f"   FLOAT32: {type_counts['FLOAT']} å€‹")
        print(f"   FLOAT16: {type_counts['FLOAT16']} å€‹")
        print(f"   INT64: {type_counts['INT64']} å€‹")
        print(f"   å…¶ä»–: {type_counts['OTHER']} å€‹")
        print(f"ğŸ“Š åˆå§‹åŒ–å™¨çµ±è¨ˆ:")
        print(f"   FLOAT32: {init_fp32} å€‹")
        print(f"   FLOAT16: {init_fp16} å€‹")
        
        # æª¢æŸ¥è¼¸å…¥è¼¸å‡ºé¡å‹
        print(f"ğŸ“Š è¼¸å…¥è¼¸å‡ºæª¢æŸ¥:")
        for inp in graph.input:
            type_name = TensorProto.DataType.Name(inp.type.tensor_type.elem_type)
            print(f"   è¼¸å…¥ {inp.name}: {type_name}")
        
        for out in graph.output:
            type_name = TensorProto.DataType.Name(out.type.tensor_type.elem_type)
            print(f"   è¼¸å‡º {out.name}: {type_name}")
        
        # æª¢æŸ¥å•é¡Œç¯€é»
        problem_nodes = []
        for vi in graph.value_info:
            if 'concat_output_cast_0' in vi.name and vi.type.tensor_type.elem_type == TensorProto.FLOAT16:
                problem_nodes.append(vi.name)
        
        if problem_nodes:
            print(f"âš ï¸  ä»æœ‰å•é¡Œçš„ç¯€é»: {problem_nodes}")
            return False
        else:
            print(f"âœ… æœªç™¼ç¾æ˜é¡¯çš„é¡å‹å•é¡Œ")
            return True
            
    except Exception as e:
        print(f"âŒ é©—è­‰å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¿«é€Ÿ FP16 æ··åˆç²¾åº¦æ¨¡å‹ä¿®å¾©å·¥å…·")
    print("å°ˆé–€è§£æ±º Concat ç¯€é»é¡å‹ä¸åŒ¹é…å•é¡Œ")
    print("=" * 60)
    
    # åŸ·è¡Œä¿®å¾©
    success = quick_fix_fp16_model()
    
    if success:
        # é©—è­‰çµæœ
        verify_success = verify_mixed_precision_model()
        
        if verify_success:
            print(f"\nğŸ‰ æ¨¡å‹ä¿®å¾©æˆåŠŸï¼")
            print(f"ç¾åœ¨å¯ä»¥æ¸¬è©¦ CUDA æ¨è«–äº†ã€‚")
            print(f"\nåŸ·è¡ŒæŒ‡ä»¤æ¸¬è©¦ï¼š")
            print(f"cd /circ330/forgithub/VisualFusion_libtorch/Onnx && ./main")
        else:
            print(f"\nâš ï¸  æ¨¡å‹ä¿®å¾©å®Œæˆï¼Œä½†é©—è­‰ç™¼ç¾å•é¡Œ")
            print(f"å»ºè­°å…ˆç”¨ CPU æ¨¡å¼æ¸¬è©¦")
    else:
        print(f"\nâŒ æ¨¡å‹ä¿®å¾©å¤±æ•—")
        print(f"å»ºè­°ä½¿ç”¨ FP32 æ¨¡å‹æˆ– CPU æ¨¡å¼")
