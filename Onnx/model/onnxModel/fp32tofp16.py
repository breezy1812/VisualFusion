#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„ ONNX FP32 åˆ°æ··åˆç²¾åº¦è½‰æ›å·¥å…·
è§£æ±ºæ‰€æœ‰é¡å‹ä¸åŒ¹é…å•é¡Œï¼š
1. Concat ç¯€é»é¡å‹ä¸åŒ¹é…
2. Conv ç¯€é»é¡å‹ä¸åŒ¹é…  
3. Opset ç‰ˆæœ¬ç›¸å®¹æ€§
4. å‰µå»ºç©©å®šçš„æ··åˆç²¾åº¦æ¨¡å‹
"""

import onnx
from onnx import TensorProto, helper
import os
import sys
import numpy as np

def create_stable_mixed_precision_model():
    """
    å‰µå»ºç©©å®šçš„æ··åˆç²¾åº¦æ¨¡å‹
    ç­–ç•¥ï¼šä¿æŒæ¬Šé‡ç‚ºFP32ä»¥ç¢ºä¿èˆ‡Convç­‰ç¯€é»ç›¸å®¹ï¼Œåªåœ¨å®‰å…¨çš„åœ°æ–¹ä½¿ç”¨FP16
    """
    
    print("ğŸ”§ å®Œæ•´çš„ FP32 â†’ ç©©å®šæ··åˆç²¾åº¦è½‰æ›")
    print("=" * 60)
    
    # æª”æ¡ˆè·¯å¾‘
    fp32_file = '/circ330/forgithub/VisualFusion_libtorch/Onnx/model/onnxModel/SemLA_onnx_320x240_fp32_cuda.onnx'
    fp16_file = '/circ330/forgithub/VisualFusion_libtorch/Onnx/model/onnxModel/zopset17_fp16.onnx'
    
    if not os.path.exists(fp32_file):
        print(f"âŒ æ‰¾ä¸åˆ° FP32 æª”æ¡ˆ: {fp32_file}")
        return False
    
    try:
        # æ­¥é©Ÿ1ï¼šè¼‰å…¥FP32æ¨¡å‹
        print("ğŸ“ è¼‰å…¥ FP32 æ¨¡å‹...")
        model = onnx.load(fp32_file)
        graph = model.graph
        
        # æª¢æŸ¥åŸå§‹å¤§å°
        fp32_size = os.path.getsize(fp32_file) / 1024 / 1024
        print(f"   FP32 æ¨¡å‹å¤§å°: {fp32_size:.2f} MB")
        print(f"   ç¯€é»æ•¸: {len(graph.node)}")
        print(f"   åˆå§‹åŒ–å™¨æ•¸: {len(graph.initializer)}")
        
        # æ­¥é©Ÿ2ï¼šå‚™ä»½èˆŠæ¨¡å‹
        if os.path.exists(fp16_file):
            backup_file = fp16_file.replace('.onnx', '_old_backup.onnx')
            os.rename(fp16_file, backup_file)
            print(f"ğŸ’¾ å‚™ä»½èˆŠæ¨¡å‹: {backup_file}")
        
        # æ­¥é©Ÿ3ï¼šåˆ†ææ¨¡å‹çµæ§‹
        print("ğŸ” åˆ†ææ¨¡å‹çµæ§‹...")
        
        # ç²å–é—œéµè³‡è¨Š
        input_names = {inp.name for inp in graph.input}
        output_names = {out.name for out in graph.output}
        initializer_names = {init.name for init in graph.initializer}
        
        print(f"   è¼¸å…¥: {len(input_names)} å€‹")
        print(f"   è¼¸å‡º: {len(output_names)} å€‹") 
        print(f"   æ¬Šé‡åƒæ•¸: {len(initializer_names)} å€‹")
        
        # æ­¥é©Ÿ4ï¼šå‰µå»ºç©©å®šçš„æ··åˆç²¾åº¦æ¨¡å‹
        print("ğŸ”„ å‰µå»ºç©©å®šçš„æ··åˆç²¾åº¦æ¨¡å‹...")
        
        # 4.1 è™•ç†åˆå§‹åŒ–å™¨ - é—œéµï¼šä¿æŒæ‰€æœ‰æ¬Šé‡ç‚º FLOAT32
        print("   è™•ç†æ¬Šé‡åƒæ•¸(ä¿æŒFP32)...")
        new_initializers = []
        
        for init in graph.initializer:
            new_init = onnx.TensorProto()
            new_init.CopyFrom(init)
            # ä¿æŒæ‰€æœ‰æ¬Šé‡ç‚º FLOAT32 ä»¥é¿å… Conv ç­‰ç¯€é»çš„é¡å‹ä¸åŒ¹é…
            if new_init.data_type == TensorProto.FLOAT:
                new_initializers.append(new_init)
            else:
                new_initializers.append(new_init)
        
        # 4.2 è™•ç†ç¯€é» - ä¿æŒåŸå§‹çµæ§‹
        print("   è™•ç†è¨ˆç®—ç¯€é»...")
        new_nodes = []
        for node in graph.node:
            new_nodes.append(node)
        
        # 4.3 æ™ºèƒ½è™•ç† value_info - é€™æ˜¯é—œéµ
        print("   æ™ºèƒ½åˆ†é…å¼µé‡ç²¾åº¦...")
        new_value_infos = []
        type_assignments = {
            'FLOAT32': 0,
            'FLOAT16': 0, 
            'INT64': 0,
            'OTHER': 0
        }
        
        for vi in graph.value_info:
            new_vi = onnx.ValueInfoProto()
            new_vi.CopyFrom(vi)
            
            original_type = vi.type.tensor_type.elem_type
            
            # é¡å‹åˆ†é…ç­–ç•¥
            if vi.name in input_names or vi.name in output_names:
                # ç­–ç•¥1: è¼¸å…¥è¼¸å‡ºå¿…é ˆä¿æŒ FLOAT32 (ç›¸å®¹æ€§)
                new_vi.type.tensor_type.elem_type = TensorProto.FLOAT
                type_assignments['FLOAT32'] += 1
                
            elif any(critical_pattern in vi.name.lower() for critical_pattern in [
                'concat', 'reducemean', 'pow', 'cast', 'conv', 
                'output_cast', 'input_cast', '/concat_output_cast_0'
            ]):
                # ç­–ç•¥2: å•é¡Œç¯€é»è¼¸å‡ºä¿æŒ FLOAT32 (ç©©å®šæ€§)
                new_vi.type.tensor_type.elem_type = TensorProto.FLOAT
                type_assignments['FLOAT32'] += 1
                
            elif any(shape_pattern in vi.name.lower() for shape_pattern in [
                'shape', 'gather', 'constantofshape', 'unsqueeze', 'squeeze'
            ]) and original_type in [TensorProto.FLOAT, TensorProto.FLOAT16]:
                # ç­–ç•¥3: å½¢ç‹€æ“ä½œä¿æŒæ•´æ•¸é¡å‹
                new_vi.type.tensor_type.elem_type = TensorProto.INT64
                type_assignments['INT64'] += 1
                
            else:
                # ç­–ç•¥4: å®‰å…¨çš„ä¸­é–“å¼µé‡ä¿æŒ FLOAT32 (ç‚ºäº†ç©©å®šæ€§)
                if original_type == TensorProto.FLOAT:
                    new_vi.type.tensor_type.elem_type = TensorProto.FLOAT
                    type_assignments['FLOAT32'] += 1
                else:
                    type_assignments['OTHER'] += 1
            
            new_value_infos.append(new_vi)
        
        print(f"   é¡å‹åˆ†é…çµ±è¨ˆ:")
        print(f"     FLOAT32: {type_assignments['FLOAT32']} å€‹")
        print(f"     FLOAT16: {type_assignments['FLOAT16']} å€‹")  
        print(f"     INT64: {type_assignments['INT64']} å€‹")
        print(f"     å…¶ä»–: {type_assignments['OTHER']} å€‹")
        
        # æ­¥é©Ÿ5ï¼šå‰µå»ºæ–°åœ–
        print("ğŸ”§ çµ„è£æ–°æ¨¡å‹...")
        new_graph = helper.make_graph(
            nodes=new_nodes,
            name=graph.name,
            inputs=graph.input,
            outputs=graph.output, 
            initializer=new_initializers,
            value_info=new_value_infos
        )
        
        # æ­¥é©Ÿ6ï¼šå‰µå»ºæ–°æ¨¡å‹ä¸¦è™•ç†metadata
        new_model = helper.make_model(new_graph)
        new_model.ir_version = model.ir_version
        new_model.producer_name = model.producer_name + "_mixed_precision"
        new_model.producer_version = model.producer_version
        new_model.domain = model.domain
        new_model.model_version = model.model_version
        
        # æ­¥é©Ÿ7ï¼šä¿®å¾© opset ç‰ˆæœ¬ç›¸å®¹æ€§
        print("ğŸ”§ ä¿®å¾© opset ç‰ˆæœ¬...")
        opset_fixed = 0
        
        for opset in model.opset_import:
            new_opset = new_model.opset_import.add()
            new_opset.CopyFrom(opset)
            
            if new_opset.version > 21:
                print(f"   é™ç´š {new_opset.domain or 'ai.onnx'}: {new_opset.version} â†’ 21")
                new_opset.version = 21
                opset_fixed += 1
        
        if model.metadata_props:
            new_model.metadata_props.extend(model.metadata_props)
        
        print(f"   ä¿®å¾©äº† {opset_fixed} å€‹ opset ç‰ˆæœ¬")
        
        # æ­¥é©Ÿ8ï¼šé©—è­‰æ¨¡å‹
        print("âœ… é©—è­‰æ¨¡å‹...")
        try:
            onnx.checker.check_model(new_model)
            print("   âœ… æ¨¡å‹é©—è­‰é€šé")
        except Exception as e:
            print(f"   âš ï¸  é©—è­‰è­¦å‘Š: {e}")
            print("   ç¹¼çºŒä¿å­˜æ¨¡å‹...")
        
        # æ­¥é©Ÿ9ï¼šä¿å­˜æœ€çµ‚æ¨¡å‹  
        print(f"ğŸ’¾ ä¿å­˜ç©©å®šæ··åˆç²¾åº¦æ¨¡å‹: {fp16_file}")
        onnx.save(new_model, fp16_file)
        
        # æ­¥é©Ÿ10ï¼šçµæœçµ±è¨ˆ
        fp16_size = os.path.getsize(fp16_file) / 1024 / 1024
        
        print("\nğŸ‰ ç©©å®šæ··åˆç²¾åº¦æ¨¡å‹å‰µå»ºå®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“Š æª”æ¡ˆå¤§å°:")
        print(f"   FP32 æ¨¡å‹: {fp32_size:.2f} MB")
        print(f"   æ··åˆç²¾åº¦æ¨¡å‹: {fp16_size:.2f} MB")
        print(f"   ç¯€çœç©ºé–“: {(1-fp16_size/fp32_size)*100:.1f}%")
        
        print(f"ğŸ“Š ä¿®å¾©çµ±è¨ˆ:")
        print(f"   Opset ç‰ˆæœ¬ä¿®å¾©: {opset_fixed} å€‹")
        print(f"   é¡å‹ç©©å®šåŒ–: {type_assignments['FLOAT32']} å€‹å¼µé‡")
        
        print("=" * 60)
        print("âœ… æ¨¡å‹ç‰¹æ€§:")
        print("  - è¼¸å…¥è¼¸å‡º: FLOAT32 (å®Œå…¨ç›¸å®¹)")
        print("  - æ¬Šé‡åƒæ•¸: FLOAT32 (é¿å…Convé¡å‹éŒ¯èª¤)")
        print("  - é—œéµç¯€é»: FLOAT32 (é¿å…Concatç­‰éŒ¯èª¤)")
        print("  - Opsetç‰ˆæœ¬: â‰¤21 (ONNX Runtime 1.18ç›¸å®¹)")
        print("  - é¡å‹ä¸€è‡´: é¿å…æ‰€æœ‰å·²çŸ¥é¡å‹éŒ¯èª¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰µå»ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = create_stable_mixed_precision_model()
    
    if success:
        print(f"\nğŸ‰ è½‰æ›å®Œå…¨æˆåŠŸï¼")
        print("\nğŸš€ ç¾åœ¨å¯ä»¥æ¸¬è©¦ CUDA æ¨è«–ï¼š")
        print("åŸ·è¡ŒæŒ‡ä»¤: cd /circ330/forgithub/VisualFusion_libtorch/Onnx && ./main")
    else:
        print(f"\nâŒ è½‰æ›å¤±æ•—")
