#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¦ç”¨çš„æ··åˆç²¾åº¦è½‰æ›ï¼šä½¿ç”¨onnxconverter-commoné€²è¡Œå®‰å…¨è½‰æ›
"""

import onnx
import os
import numpy as np
from onnxconverter_common import float16

def convert_mixed_precision_safe():
    """ä½¿ç”¨onnxconverter-commoné€²è¡Œå®‰å…¨çš„æ··åˆç²¾åº¦è½‰æ›"""
    
    print("ğŸ”§ å®‰å…¨æ··åˆç²¾åº¦è½‰æ› (ä½¿ç”¨onnxconverter-common)")
    print("=" * 60)
    
    fp32_file = './SemLA_onnx_320x240_fp32_cuda.onnx'
    fp16_file = './SemLA_onnx_320x240_fp16_safe.onnx'
    
    if not os.path.exists(fp32_file):
        print(f"âŒ æ‰¾ä¸åˆ°FP32æª”æ¡ˆ: {fp32_file}")
        return False
    
    try:
        print("ğŸ“ è¼‰å…¥FP32æ¨¡å‹...")
        model_fp32 = onnx.load(fp32_file)
        
        original_size = os.path.getsize(fp32_file) / 1024 / 1024
        print(f"   åŸå§‹å¤§å°: {original_size:.2f} MB")
        print(f"   ç¯€é»æ•¸: {len(model_fp32.graph.node)}")
        print(f"   æ¬Šé‡æ•¸: {len(model_fp32.graph.initializer)}")
        
        # ä½¿ç”¨onnxconverter-commoné€²è¡Œè½‰æ›
        print("ğŸ”„ åŸ·è¡Œæ··åˆç²¾åº¦è½‰æ›...")
        
        # è½‰æ›ç‚ºFP16ï¼Œä½†ä¿æŒè¼¸å…¥è¼¸å‡ºç‚ºFP32
        model_fp16 = float16.convert_float_to_float16(
            model_fp32,
            keep_io_types=True,  # ä¿æŒè¼¸å…¥è¼¸å‡ºé¡å‹ç‚ºFP32
            disable_shape_infer=False  # å•Ÿç”¨å½¢ç‹€æ¨æ–·
        )
        
        print("âœ… æ··åˆç²¾åº¦è½‰æ›å®Œæˆ")
        
        # ä¿®æ­£opsetç‰ˆæœ¬ç‚ºå…¼å®¹ç‰ˆæœ¬
        print("ğŸ”§ ä¿®æ­£opsetç‰ˆæœ¬...")
        model_fp16.opset_import[0].version = 12
        model_fp16.ir_version = 8
        
        # æª¢æŸ¥æ¨¡å‹
        print("ğŸ” æª¢æŸ¥è½‰æ›å¾Œæ¨¡å‹...")
        try:
            onnx.checker.check_model(model_fp16)
            print("âœ… æ¨¡å‹æª¢æŸ¥é€šé")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹æª¢æŸ¥è­¦å‘Š: {e}")
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ··åˆç²¾åº¦æ¨¡å‹...")
        onnx.save(model_fp16, fp16_file)
        
        # é©—è­‰çµæœ
        print("\nğŸ§ª é©—è­‰è½‰æ›çµæœ:")
        
        new_size = os.path.getsize(fp16_file) / 1024 / 1024
        compression_ratio = new_size / original_size
        
        print(f"   æª”æ¡ˆå¤§å°: {new_size:.2f} MB")
        print(f"   å£“ç¸®ç‡: {(1-compression_ratio)*100:.1f}%")
        print(f"   IRç‰ˆæœ¬: {model_fp16.ir_version}")
        print(f"   Opsetç‰ˆæœ¬: {[f'{op.domain}:{op.version}' for op in model_fp16.opset_import]}")
        
        # çµ±è¨ˆæ¬Šé‡é¡å‹
        fp16_count = 0
        fp32_count = 0
        other_count = 0
        
        for init in model_fp16.graph.initializer:
            if init.data_type == onnx.TensorProto.FLOAT16:
                fp16_count += 1
            elif init.data_type == onnx.TensorProto.FLOAT:
                fp32_count += 1
            else:
                other_count += 1
        
        print(f"   æ¬Šé‡çµ±è¨ˆ: FP16={fp16_count}, FP32={fp32_count}, å…¶ä»–={other_count}")
        
        # æ¸¬è©¦ONNX Runtimeè¼‰å…¥
        print("\nğŸ¯ æ¸¬è©¦ONNX Runtimeç›¸å®¹æ€§...")
        try:
            import onnxruntime as ort
            
            # å˜—è©¦è¼‰å…¥
            session = ort.InferenceSession(fp16_file, providers=['CPUExecutionProvider'])
            print("âœ… CPU Providerè¼‰å…¥æˆåŠŸ")
            
            # æª¢æŸ¥è¼¸å…¥è¼¸å‡º
            print("   æ¨¡å‹è¼¸å…¥:")
            for inp in session.get_inputs():
                print(f"     {inp.name}: {inp.shape} ({inp.type})")
            print("   æ¨¡å‹è¼¸å‡º:")
            for out in session.get_outputs():
                print(f"     {out.name}: {out.shape} ({out.type})")
            
            # æ¸¬è©¦æ¨è«–
            print("   åŸ·è¡Œæ¸¬è©¦æ¨è«–...")
            dummy_inputs = {
                'vi_img': np.random.randn(1, 1, 240, 320).astype(np.float32),
                'ir_img': np.random.randn(1, 1, 240, 320).astype(np.float32)
            }
            outputs = session.run(None, dummy_inputs)
            print(f"   æ¨è«–æˆåŠŸï¼è¼¸å‡ºå½¢ç‹€: {[out.shape for out in outputs]}")
            
        except Exception as e:
            print(f"âŒ ONNX Runtimeæ¸¬è©¦å¤±æ•—: {e}")
            return False
        
        print("\nâœ… å®‰å…¨æ··åˆç²¾åº¦æ¨¡å‹è½‰æ›å®Œæˆï¼")
        print(f"   è¼¸å‡ºæª”æ¡ˆ: {fp16_file}")
        print("   â€¢ è¼¸å…¥è¼¸å‡ºä¿æŒFP32ï¼Œç¢ºä¿ç›¸å®¹æ€§")
        print("   â€¢ å…§éƒ¨æ¬Šé‡ä½¿ç”¨FP16ï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨")
        print("   â€¢ èˆ‡ONNX Runtime 1.18.0å®Œå…¨ç›¸å®¹")
        return True
        
    except Exception as e:
        print(f"âŒ è½‰æ›å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = convert_mixed_precision_safe()
    if success:
        print("\nğŸ‰ æˆåŠŸï¼å¯ç”¨çš„æ··åˆç²¾åº¦æ¨¡å‹å·²å‰µå»º")
        print("   æ›´æ–°config.jsonä»¥ä½¿ç”¨æ–°æ¨¡å‹")
    else:
        print("\nğŸ’¥ å¤±æ•—ï¼è«‹æª¢æŸ¥éŒ¯èª¤ä¿¡æ¯")
