#include "../include/core_image_align_onnx.h"
#include <algorithm>
#include <cmath>
#include <iostream>
#include <chrono>
#include <fstream>
#include <cstdlib>  // ç‚ºsetenvå‡½æ•¸æ·»åŠ 
#include <onnxruntime_cxx_api.h>

// å˜—è©¦åŒ…å«CUDAé ­æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è·³é
#ifdef __CUDACC__
#include <cuda_runtime.h>
#endif

namespace core {

class ImageAlignONNXImpl : public ImageAlignONNX {
private:
    Param param_;
    Ort::Env env_;
    Ort::SessionOptions session_options_;
    std::unique_ptr<Ort::Session> session_;
    std::unique_ptr<Ort::MemoryInfo> memory_info_;
    std::vector<const char*> input_names_;
    std::vector<const char*> output_names_;
    std::vector<Ort::AllocatedStringPtr> input_names_ptrs_;
    std::vector<Ort::AllocatedStringPtr> output_names_ptrs_;
    
    // CSV logging for inference times
    std::ofstream csv_file_;
    int inference_count_ = 0;
    std::string current_image_name_ = "";  // æ–°å¢ï¼šç•¶å‰è™•ç†çš„åœ–ç‰‡åç¨±
    
    // Smart warmup for ONNX: åˆå§‹åŒ– provider ä½†ä¸å½±éŸ¿ç²¾åº¦
    void smart_warmup_onnx() {
        std::cout << "Smart warmup for ONNX providers initialization..." << std::endl;
        
        cv::Mat eo = cv::Mat::ones(param_.pred_height, param_.pred_width, CV_8UC1) * 128;
        cv::Mat ir = cv::Mat::ones(param_.pred_height, param_.pred_width, CV_8UC1) * 128;
        
        const auto t0 = std::chrono::high_resolution_clock::now();
        
        // åªåŸ·è¡Œä¸€æ¬¡æ¨ç†ä¾†åˆå§‹åŒ– ONNX providers
        std::vector<cv::Point2i> dummy_eo_mkpts, dummy_ir_mkpts;
        try {
            pred_cpu(eo, ir, dummy_eo_mkpts, dummy_ir_mkpts);
        } catch (const std::exception& e) {
            std::cerr << "Warning: Smart warmup failed: " << e.what() << std::endl;
        }
        
        // é‡æ–°å‰µå»º session ä»¥æ¸…é™¤å…§éƒ¨ç‹€æ…‹ï¼Œä¿æŒç¬¬ä¸€æ¬¡æ¨ç†çš„ç²¾åº¦
        std::cout << "Recreating ONNX session to maintain first-inference precision..." << std::endl;
        session_.reset();
        session_ = std::make_unique<Ort::Session>(env_, param_.model_path.c_str(), session_options_);
        
        // é‡æ–°åˆå§‹åŒ– input/output namesï¼ˆé€™å¾ˆé‡è¦ï¼ï¼‰
        input_names_.clear();
        output_names_.clear();
        input_names_ptrs_.clear();
        output_names_ptrs_.clear();
        
        Ort::AllocatorWithDefaultOptions allocator;
        
        // Get input names
        size_t num_input_nodes = session_->GetInputCount();
        for (size_t i = 0; i < num_input_nodes; i++) {
            auto input_name = session_->GetInputNameAllocated(i, allocator);
            input_names_ptrs_.push_back(std::move(input_name));
            input_names_.push_back(input_names_ptrs_.back().get());
        }
        
        // Get output names
        size_t num_output_nodes = session_->GetOutputCount();
        for (size_t i = 0; i < num_output_nodes; i++) {
            auto output_name = session_->GetOutputNameAllocated(i, allocator);
            output_names_ptrs_.push_back(std::move(output_name));
            output_names_.push_back(output_names_ptrs_.back().get());
        }
        
        const auto t1 = std::chrono::high_resolution_clock::now();
        const auto dt = std::chrono::duration_cast<std::chrono::milliseconds>(t1 - t0);
        std::cout << "Smart warmup completed in " << dt.count() << " ms" << std::endl;
    }

public:
    ImageAlignONNXImpl(const Param& param) : param_(param), env_(ORT_LOGGING_LEVEL_WARNING, "ImageAlign") {
        // è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿èˆ‡LibTorch C++ä»£ç¢¼ä¸€è‡´çš„æ¨ç†çµæœ
        std::cout << "debug: Setting deterministic seeds for ONNX inference..." << std::endl;
        std::srand(1);  // èˆ‡LibTorch C++ä¸€è‡´ï¼šä½¿ç”¨ç¨®å­1
        srand(1);       // ç¢ºä¿æ‰€æœ‰Céš¨æ©Ÿå‡½æ•¸éƒ½ä½¿ç”¨ç›¸åŒç¨®å­
        std::cout << "debug: Random seeds and environment configured for deterministic ONNX inference (seed=1, matching LibTorch C++)" << std::endl;
        
        // Initialize CSV file for logging inference times
        csv_file_.open("onnx_inference_times.csv", std::ios::app);
        if (!csv_file_.is_open()) {
            std::cerr << "Warning: Could not open CSV file for writing inference times" << std::endl;
        } else {
            // Write header if file is empty/new
            csv_file_.seekp(0, std::ios::end);
            if (csv_file_.tellp() == 0) {
                csv_file_ << "Image_Name,Inference_Time_Seconds,Features_Count" << std::endl;
            }
        }
        
        // æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if (!std::experimental::filesystem::exists(param_.model_path)) {
            std::cerr << "FATAL ERROR: Model file not found: " << param_.model_path << std::endl;
            throw std::runtime_error("ONNX model file not found: " + param_.model_path);
        }
        
        try {
            // ===== ğŸ”’ ONNX Runtime ç¢ºå®šæ€§èˆ‡ç²¾åº¦è¨­å®š =====
            std::cout << "debug: Configuring ONNX Runtime for FP32 precision and determinism..." << std::endl;
            
            // å¼·åˆ¶å–®åŸ·è¡Œç·’åŸ·è¡Œï¼Œé¿å…ä¸¦è¡Œå°è‡´çš„éç¢ºå®šæ€§
            session_options_.SetIntraOpNumThreads(1);                    
            session_options_.SetInterOpNumThreads(1);                    
            
            // è¨­å®šç¢ºå®šæ€§åŸ·è¡Œæ¨¡å¼
            session_options_.SetExecutionMode(ExecutionMode::ORT_SEQUENTIAL);
            
            // ç¦ç”¨åœ–å„ªåŒ–ä»¥ç¢ºä¿ç¢ºå®šæ€§
            session_options_.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_DISABLE_ALL);
            
            // è¨­å®šæ—¥å¿—ç­‰ç´š
            session_options_.SetLogSeverityLevel(3);
            
            // é¡å¤–çš„ç¢ºå®šæ€§è¨­å®šï¼ˆæ ¹æ“šONNX Runtimeç‰ˆæœ¬å¯èƒ½ä¸å¯ç”¨ï¼‰
            try {
                session_options_.DisableMemPattern();     // ç¦ç”¨è¨˜æ†¶é«”æ¨¡å¼å„ªåŒ–
            } catch (...) {
                std::cout << "debug: DisableMemPattern not available in this ONNX Runtime version" << std::endl;
            }
            
            try {
                session_options_.DisableCpuMemArena();    // ç¦ç”¨CPUè¨˜æ†¶é«”æ± 
            } catch (...) {
                std::cout << "debug: DisableCpuMemArena not available in this ONNX Runtime version" << std::endl;
            }
            
            // Check if CUDA is requested and available
            std::cout << "debug: Attempting to initialize ONNX Runtime with device: " << param_.device << std::endl;
            
            bool use_cuda = false;
            if (param_.device == "cuda") {  // ä½¿ç”¨CUDA
                std::cout << "debug: Adding CUDA execution provider with FP32 and deterministic settings..." << std::endl;
                
                // ===== ğŸš« ç¦ç”¨ TF32ï¼Œå¼·åˆ¶ä½¿ç”¨ FP32 =====
                OrtCUDAProviderOptions cuda_options{};
                cuda_options.device_id = 0;
                
                // è¨­å®šç¢ºå®šæ€§ç®—æ³•é¸æ“‡
                cuda_options.cudnn_conv_algo_search = OrtCudnnConvAlgoSearchHeuristic;  
                cuda_options.do_copy_in_default_stream = 1;    // å¼·åˆ¶åŒæ­¥è¤‡è£½
                
                // ğŸ”‘ é—œéµè¨­å®šï¼šç¦ç”¨ TF32ï¼Œå¼·åˆ¶ä½¿ç”¨ FP32
                // é€éç’°å¢ƒè®Šæ•¸ç¦ç”¨ TF32ï¼ˆé€™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼‰
                setenv("NVIDIA_TF32_OVERRIDE", "0", 1);
                std::cout << "âœ… Set NVIDIA_TF32_OVERRIDE=0 to disable TF32" << std::endl;
                
                session_options_.AppendExecutionProvider_CUDA(cuda_options);
                use_cuda = true;
                std::cout << "âœ… CUDA execution provider added with FP32 precision (TF32 disabled)" << std::endl;
            } else {
                std::cout << "debug: Using CPU execution provider" << std::endl;
            }
            
            session_ = std::make_unique<Ort::Session>(env_, param_.model_path.c_str(), session_options_);
            
            std::cout << "âœ… ONNX Runtime session created with:" << std::endl;
            std::cout << "  - FP32 precision (TF32 disabled)" << std::endl;
            std::cout << "  - Deterministic mode enabled" << std::endl;
            std::cout << "  - Single-threaded execution" << std::endl;
            
            // Use CPU memory allocator for input preparation
            memory_info_ = std::make_unique<Ort::MemoryInfo>(Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault));
            
            // Get input/output names
            Ort::AllocatorWithDefaultOptions allocator;
            
            // Get input names
            size_t num_input_nodes = session_->GetInputCount();
            for (size_t i = 0; i < num_input_nodes; i++) {
                auto input_name = session_->GetInputNameAllocated(i, allocator);
                input_names_ptrs_.push_back(std::move(input_name));
                input_names_.push_back(input_names_ptrs_.back().get());
            }
            
            // Get output names
            size_t num_output_nodes = session_->GetOutputCount();
            for (size_t i = 0; i < num_output_nodes; i++) {
                auto output_name = session_->GetOutputNameAllocated(i, allocator);
                output_names_ptrs_.push_back(std::move(output_name));
                output_names_.push_back(output_names_ptrs_.back().get());
            }
            
            std::cout << "Successfully loaded ONNX model with " << num_input_nodes << " inputs and " << num_output_nodes << " outputs" << std::endl;
            
            // æ™ºèƒ½ warmup (å¯é¸)
            if (param_.device.compare("cuda") == 0) {
                std::cout << "ONNX CUDA model initialized without warmup to maintain first-inference precision" << std::endl;
            } else {
                std::cout << "ONNX CPU model initialized without warmup to maintain first-inference precision" << std::endl;
            }
            
        } catch (const Ort::Exception& e) {
            std::cerr << "FATAL ERROR: Failed to load ONNX model: " << e.what() << std::endl;
            throw std::runtime_error("Failed to load ONNX model: " + std::string(e.what()));
        }
    }
    
    // Destructor
    ~ImageAlignONNXImpl() {
        if (csv_file_.is_open()) {
            csv_file_.close();
            std::cout << "CSV file with ONNX inference times closed. Total inferences: " << inference_count_ << std::endl;
        }
    }

    // predict keypoints using cpu
    void pred_cpu(cv::Mat &eo, cv::Mat &ir, std::vector<cv::Point2i> &eo_mkpts, std::vector<cv::Point2i> &ir_mkpts) {
        double inference_time_seconds = 0.0;
        
        // å®Œå…¨å°æ‡‰LibTorchçš„åœ–åƒé è™•ç†
        if (eo.channels() != 1 || ir.channels() != 1) {
            throw std::runtime_error("ImageAlignONNXImpl::pred: eo and ir must be single channel images");
        }
        
        // æ ¹æ“š pred_mode æ±ºå®šè¼¸å…¥å‹åˆ¥
        bool use_fp16 = (param_.pred_mode == "fp16");
        std::cout << "debug: Using " << (use_fp16 ? "FP16" : "FP32") << " input (pred_mode=" << param_.pred_mode << ")" << std::endl;
        
        cv::Mat eo_float, ir_float;
        eo.convertTo(eo_float, CV_32F, 1.0 / 255.0, 0.0);
        ir.convertTo(ir_float, CV_32F, 1.0 / 255.0, 0.0);

        size_t input_size = param_.pred_height * param_.pred_width;
        
        // å‰µå»ºå¼µé‡å½¢ç‹€ [1, 1, H, W]
        std::vector<int64_t> input_shape = {1, 1, param_.pred_height, param_.pred_width};
        
        // å‰µå»ºè¼¸å…¥å¼µé‡å‘é‡
        std::vector<Ort::Value> inputs;
        
        if (use_fp16) {
            // ===== FP16 æ¨¡å¼ï¼šå°‡è¼¸å…¥è½‰æ›ç‚º FP16 =====
            std::vector<Ort::Float16_t> eo_fp16_data(input_size);
            std::vector<Ort::Float16_t> ir_fp16_data(input_size);

            // å°‡ FP32 è½‰æ›ç‚º FP16
            const float* eo_ptr = eo_float.ptr<float>();
            const float* ir_ptr = ir_float.ptr<float>();
            
            for (size_t i = 0; i < input_size; i++) {
                eo_fp16_data[i] = Ort::Float16_t(eo_ptr[i]);
                ir_fp16_data[i] = Ort::Float16_t(ir_ptr[i]);
            }
            
            std::cout << "debug: Converted input data to FP16 format" << std::endl;

            // å‰µå»º FP16 å¼µé‡ï¼ˆæ³¨æ„ï¼šdata å¿…é ˆåœ¨ Run ä¹‹å‰æœ‰æ•ˆï¼Œæ‰€ä»¥ä½¿ç”¨éœæ…‹è®Šæ•¸ï¼‰
            static std::vector<Ort::Float16_t> eo_fp16_static, ir_fp16_static;
            eo_fp16_static = std::move(eo_fp16_data);
            ir_fp16_static = std::move(ir_fp16_data);
            
            inputs.push_back(Ort::Value::CreateTensor<Ort::Float16_t>(
                *memory_info_, eo_fp16_static.data(), input_size, input_shape.data(), 4));
            inputs.push_back(Ort::Value::CreateTensor<Ort::Float16_t>(
                *memory_info_, ir_fp16_static.data(), input_size, input_shape.data(), 4));
        } else {
            
            std::cout << "777777777777777777777777777777777777" << std::endl;
            // ===== FP32 æ¨¡å¼ï¼šä½¿ç”¨ FP32 è¼¸å…¥ =====
            std::vector<float> eo_float32_data(input_size);
            std::vector<float> ir_float32_data(input_size);

            // ä½¿ç”¨ .ptr<float>() ç›´æ¥å­˜å–è³‡æ–™
            for (size_t i = 0; i < input_size; i++) {
                eo_float32_data[i] = eo_float.ptr<float>()[i];
                ir_float32_data[i] = ir_float.ptr<float>()[i];
            }
            
            // å‰µå»º FP32 å¼µé‡ï¼ˆåŒæ¨£ä½¿ç”¨éœæ…‹è®Šæ•¸ï¼‰
            static std::vector<float> eo_fp32_static, ir_fp32_static;
            eo_fp32_static = std::move(eo_float32_data);
            ir_fp32_static = std::move(ir_float32_data);
            
            inputs.push_back(Ort::Value::CreateTensor<float>(
                *memory_info_, eo_fp32_static.data(), input_size, input_shape.data(), 4));
            inputs.push_back(Ort::Value::CreateTensor<float>(
                *memory_info_, ir_fp32_static.data(), input_size, input_shape.data(), 4));
        }

        // é–‹å§‹æ¨ç†è¨ˆæ™‚
        auto inference_start = std::chrono::high_resolution_clock::now();
        
        // å‰µå»ºé‹è¡Œé¸é …
        Ort::RunOptions run_options;
        run_options.SetRunLogSeverityLevel(3);
        
        std::cout << "debug: Running ONNX model inference (pred_mode=" << param_.pred_mode << ")..." << std::endl;
        
        // åŸ·è¡Œæ¨¡å‹æ¨ç†
        auto pred = session_->Run(run_options, input_names_.data(), inputs.data(), 2, 
                                output_names_.data(), output_names_.size());
        
        // çµæŸæ¨ç†è¨ˆæ™‚
        auto inference_end = std::chrono::high_resolution_clock::now();
        auto inference_duration = std::chrono::duration_cast<std::chrono::microseconds>(inference_end - inference_start);
        inference_time_seconds = inference_duration.count() / 1000000.0;
        
        std::cout << "ONNX Inference time (" << param_.pred_mode << "): " << inference_time_seconds << " seconds" << std::endl;

        
        // æ–°æ¨¡å‹åªè¿”å› 2 å€‹è¼¸å‡ºï¼šmkpts0 å’Œ mkpts1 (int32 é¡å‹)
        const int32_t *eo_res = pred[0].GetTensorMutableData<int32_t>();
        const int32_t *ir_res = pred[1].GetTensorMutableData<int32_t>();
        
        // ç²å–è¼¸å‡ºç¶­åº¦ [1200, 2]
        auto eo_shape = pred[0].GetTensorTypeAndShapeInfo().GetShape();
        int num_points = static_cast<int>(eo_shape[0]);  // æ‡‰è©²æ˜¯ 1200
        
        eo_mkpts.clear();
        ir_mkpts.clear();

        // éæ­·æ‰€æœ‰é»ï¼Œéæ¿¾æ‰åº§æ¨™ç‚º (0, 0) çš„ç„¡æ•ˆé»
        for (int i = 0, pt = 0; i < num_points; i++, pt += 2) {
            int eo_x = static_cast<int>(eo_res[pt]);
            int eo_y = static_cast<int>(eo_res[pt + 1]);
            int ir_x = static_cast<int>(ir_res[pt]);
            int ir_y = static_cast<int>(ir_res[pt + 1]);
            
            // è·³éåº§æ¨™ç‚º (0, 0) çš„ç„¡æ•ˆé»
            if (eo_x == 0 && eo_y == 0) {
                continue;
            }
            
            eo_mkpts.push_back(cv::Point2i(eo_x, eo_y));
            ir_mkpts.push_back(cv::Point2i(ir_x, ir_y));
        }
        
        std::cout << "Extracted " << eo_mkpts.size() << " valid feature point pairs (pred_mode=" << param_.pred_mode << ")" << std::endl;
        
        // è¨˜éŒ„æ¨ç†æ™‚é–“åˆ°CSVï¼ˆå°æ‡‰LibTorchçš„writeTimingToCSVï¼‰
        inference_count_++;
        if (csv_file_.is_open()) {
            std::string image_name = current_image_name_.empty() ? 
                "----" : current_image_name_;
            if(image_name=="----"){
                return;
            }
            csv_file_ << image_name << "," << inference_time_seconds << "," 
                     << eo_mkpts.size() << std::endl;
            csv_file_.flush();
        }
    }

    // alias for pred_cpu
    void pred(cv::Mat &eo, cv::Mat &ir, std::vector<cv::Point2i> &eo_mkpts, std::vector<cv::Point2i> &ir_mkpts) {
        pred_cpu(eo, ir, eo_mkpts, ir_mkpts);
    }

    // align with last H - èˆ‡LibTorchç‰ˆæœ¬ä¿æŒä¸€è‡´
    void align(cv::Mat &eo, cv::Mat &ir, std::vector<cv::Point2i> &eo_pts, std::vector<cv::Point2i> &ir_pts, cv::Mat &H) {
        // predict keypoints
        pred(eo, ir, eo_pts, ir_pts);
        
        // è¿”å›å–®ä½çŸ©é™£ï¼Œè®“main.cppè™•ç†homographyè¨ˆç®—ï¼ˆèˆ‡LibTorchç‰ˆæœ¬ä¸€è‡´ï¼‰
        H = cv::Mat::eye(3, 3, CV_64F);
        std::cout << "Feature point extraction complete. Found " << eo_pts.size() << " points." << std::endl;
    }

    bool align(const cv::Mat& eo, const cv::Mat& ir,
              std::vector<cv::Point2i>& eo_pts,
              std::vector<cv::Point2i>& ir_pts,
              cv::Mat& H) override {
        
        try {
            cv::Mat eo_copy = eo.clone();
            cv::Mat ir_copy = ir.clone();
            align(eo_copy, ir_copy, eo_pts, ir_pts, H);
            return !H.empty() && cv::determinant(H) > 1e-6;
        } catch (const std::exception& e) {
            std::cerr << "Error in alignment: " << e.what() << std::endl;
            return false;
        }
    }

    void set_current_image_name(const std::string& name) override {
        current_image_name_ = name;
    }
};

ImageAlignONNX::ptr ImageAlignONNX::create_instance(const Param& param) {
    return std::make_shared<ImageAlignONNXImpl>(param);
}

}
